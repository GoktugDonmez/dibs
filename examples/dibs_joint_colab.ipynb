{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e361100",
   "metadata": {},
   "source": [
    "## Example: Joint inference of $p(G, \\Theta | D)$ for Gaussian Bayes nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d0dfc",
   "metadata": {},
   "source": [
    "Setup for Google Colab. Selecting the **GPU** runtime available in Google colab will make inference significantly faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet dibs-lib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b52aa",
   "metadata": {},
   "source": [
    "DiBS translates the task of inferring the posterior over Bayesian networks into an inference problem over the continuous latent variable $Z$. This is achieved by modeling the directed acyclic graph $G$ of the Bayesian network using the generative model $p(G | Z)$. The prior $p(Z)$ enforces the acyclicity of $G$.\n",
    "Ultimately, this allows us to infer $p(G, \\Theta | D)$ (and $p(G | D)$) using off-the-shelf inference methods such as Stein Variational gradient descent (SVGD) (Liu and Wang, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8370c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.random as random\n",
    "key = random.PRNGKey(123)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5e4a",
   "metadata": {},
   "source": [
    "### Generate synthetic ground truth Bayesian network and BN model for inference\n",
    "\n",
    "`data` contains information about and observations sampled from a synthetic, ground truth causal model with `n_vars` variables. By default, the conditional distributions are linear Gaussian. The random graph model is set by `graph_prior_str`, where `er` denotes Erdos-Renyi and `sf` scale-free graphs. \n",
    "\n",
    "`graph_model` defines prior $p(G)$ and `likelihood_model` defines likelihood $p(x, \\Theta| G ) = p(\\Theta| G )p(x | G, \\Theta )$ of the BN model for which DiBS will infer the posterior.\n",
    "\n",
    "**For posterior inference of nonlinear Gaussian networks parameterized by fully-connected neural networks, use the function `make_nonlinear_gaussian_model`.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98551a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dibs.target import make_linear_gaussian_model, make_nonlinear_gaussian_model\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "key, subk = random.split(key)\n",
    "data, graph_model, likelihood_model = make_linear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "# data, graph_model, likelihood_model = make_nonlinear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "\n",
    "visualize_ground_truth(data.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e46b",
   "metadata": {},
   "source": [
    "### DiBS with SVGD\n",
    "\n",
    "Infer $p(G, \\Theta | D)$ under the prior and conditional distributions defined by the model.\n",
    "The below visualization shows the *matrix of edge probabilities* $G_\\alpha(Z^{(k)})$ implied by each transported latent particle (i.e., sample) $Z^{(k)}$ during the iterations of SVGD with DiBS. Refer to the paper for further details.\n",
    "\n",
    "To explicitly perform posterior inference of $p(G | D)$ using a closed-form marginal likelihood $p(D | G)$, use the separate, analogous class `MarginalDiBS` as demonstrated in the example notebook `dibs_marginal.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ece4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.inference import JointDiBS\n",
    "\n",
    "dibs = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "key, subk = random.split(key)\n",
    "gs, thetas = dibs.sample(key=subk, n_particles=20, steps=2000, callback_every=100, callback=dibs.visualize_callback())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dc8cc",
   "metadata": {},
   "source": [
    "### Evaluate on held-out data\n",
    "\n",
    "Form the empirical (i.e., weighted by counts) and mixture distributions (i.e., weighted by unnormalized posterior probabilities, denoted DiBS+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "dibs_mixture = dibs.get_mixture(gs, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e275383",
   "metadata": {},
   "source": [
    "Compute some evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3dc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "\n",
    "for descr, dist in [('DiBS ', dibs_empirical), ('DiBS+', dibs_mixture)]:\n",
    "    \n",
    "    eshd = expected_shd(dist=dist, g=data.g)        \n",
    "    auroc = threshold_metrics(dist=dist, g=data.g)['roc_auc']\n",
    "    negll = neg_ave_log_likelihood(dist=dist, eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    \n",
    "    print(f'{descr} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}    neg. LL {negll:5.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9f4eb",
   "metadata": {},
   "source": [
    "## Deep Ensemble Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd396de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_key = random.PRNGKey(42)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# Generate ground truth nonlinear Gaussian model\n",
    "print(\"Generating ground truth nonlinear Gaussian model...\")\n",
    "key, subk = random.split(main_key)\n",
    "data, graph_model, likelihood_model = make_nonlinear_gaussian_model(\n",
    "    key=subk, \n",
    "    n_vars=20, \n",
    "    graph_prior_str=\"sf\"\n",
    ")\n",
    "\n",
    "print(f\"Ground truth graph has {np.sum(data.g)} edges\")\n",
    "print(\"Visualizing ground truth...\")\n",
    "try:\n",
    "    visualize_ground_truth(data.g)\n",
    "except:\n",
    "    print(\"Visualization skipped (may not work in all environments)\")\n",
    "\n",
    "# Experiment parameters\n",
    "n_ensemble_runs = 20\n",
    "n_particles_svgd = 20\n",
    "n_steps = 2000\n",
    "callback_every = 500\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SETUP\")\n",
    "print(f\"  Deep Ensemble: {n_ensemble_runs} runs × 1 particle each\")\n",
    "print(f\"  SVGD: 1 run × {n_particles_svgd} particles\")\n",
    "print(f\"  Training steps: {n_steps}\")\n",
    "print(f\"  Variables: {data.x.shape[1]}\")\n",
    "print(f\"  Training samples: {data.x.shape[0]}\")\n",
    "print(f\"  Test samples: {data.x_ho.shape[0]}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Storage for results\n",
    "ensemble_results = []\n",
    "ensemble_metrics = {\n",
    "    'eshd_empirical': [],\n",
    "    'auroc_empirical': [],\n",
    "    'negll_empirical': [],\n",
    "    'eshd_mixture': [],\n",
    "    'auroc_mixture': [],\n",
    "    'negll_mixture': [],\n",
    "    'training_time': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEEP ENSEMBLE APPROACH (20 runs × 1 particle)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Deep Ensemble: 20 runs with 1 particle each\n",
    "for run_idx in range(n_ensemble_runs):\n",
    "    print(f\"\\nRun {run_idx + 1}/{n_ensemble_runs}\")\n",
    "    \n",
    "    # Use different seed for each run\n",
    "    key, subk = random.split(key)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create DiBS instance\n",
    "    dibs = JointDiBS(\n",
    "        x=data.x, \n",
    "        interv_mask=None, \n",
    "        graph_model=graph_model, \n",
    "        likelihood_model=likelihood_model\n",
    "    )\n",
    "    \n",
    "    # Sample with 1 particle\n",
    "    gs, thetas = dibs.sample(\n",
    "        key=subk, \n",
    "        n_particles=1, \n",
    "        steps=n_steps, \n",
    "        callback_every=callback_every\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Get distributions\n",
    "    dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "    dibs_mixture = dibs.get_mixture(gs, thetas)\n",
    "    \n",
    "    # Compute metrics\n",
    "    # Empirical\n",
    "    eshd_emp = expected_shd(dist=dibs_empirical, g=data.g)\n",
    "    auroc_emp = threshold_metrics(dist=dibs_empirical, g=data.g)['roc_auc']\n",
    "    negll_emp = neg_ave_log_likelihood(\n",
    "        dist=dibs_empirical, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Mixture\n",
    "    eshd_mix = expected_shd(dist=dibs_mixture, g=data.g)\n",
    "    auroc_mix = threshold_metrics(dist=dibs_mixture, g=data.g)['roc_auc']\n",
    "    negll_mix = neg_ave_log_likelihood(\n",
    "        dist=dibs_mixture, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    run_result = {\n",
    "        'run_idx': run_idx,\n",
    "        'eshd_empirical': eshd_emp,\n",
    "        'auroc_empirical': auroc_emp,\n",
    "        'negll_empirical': negll_emp,\n",
    "        'eshd_mixture': eshd_mix,\n",
    "        'auroc_mixture': auroc_mix,\n",
    "        'negll_mixture': negll_mix,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    ensemble_results.append(run_result)\n",
    "    \n",
    "    # Also store in lists for easy aggregation\n",
    "    ensemble_metrics['eshd_empirical'].append(eshd_emp)\n",
    "    ensemble_metrics['auroc_empirical'].append(auroc_emp)\n",
    "    ensemble_metrics['negll_empirical'].append(negll_emp)\n",
    "    ensemble_metrics['eshd_mixture'].append(eshd_mix)\n",
    "    ensemble_metrics['auroc_mixture'].append(auroc_mix)\n",
    "    ensemble_metrics['negll_mixture'].append(negll_mix)\n",
    "    ensemble_metrics['training_time'].append(training_time)\n",
    "    \n",
    "    print(f\"  Empirical - E-SHD: {eshd_emp:5.2f}, AUROC: {auroc_emp:5.3f}, NegLL: {negll_emp:6.2f}\")\n",
    "    print(f\"  Mixture   - E-SHD: {eshd_mix:5.2f}, AUROC: {auroc_mix:5.3f}, NegLL: {negll_mix:6.2f}\")\n",
    "    print(f\"  Time: {training_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVGD APPROACH (1 run × 20 particles)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SVGD: 1 run with 20 particles\n",
    "key, subk = random.split(key)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dibs_svgd = JointDiBS(\n",
    "    x=data.x, \n",
    "    interv_mask=None, \n",
    "    graph_model=graph_model, \n",
    "    likelihood_model=likelihood_model\n",
    ")\n",
    "\n",
    "gs_svgd, thetas_svgd = dibs_svgd.sample(\n",
    "    key=subk, \n",
    "    n_particles=n_particles_svgd, \n",
    "    steps=n_steps, \n",
    "    callback_every=callback_every\n",
    ")\n",
    "\n",
    "svgd_training_time = time.time() - start_time\n",
    "\n",
    "# Get distributions\n",
    "svgd_empirical = dibs_svgd.get_empirical(gs_svgd, thetas_svgd)\n",
    "svgd_mixture = dibs_svgd.get_mixture(gs_svgd, thetas_svgd)\n",
    "\n",
    "# Compute metrics\n",
    "# Empirical\n",
    "svgd_eshd_emp = expected_shd(dist=svgd_empirical, g=data.g)\n",
    "svgd_auroc_emp = threshold_metrics(dist=svgd_empirical, g=data.g)['roc_auc']\n",
    "svgd_negll_emp = neg_ave_log_likelihood(\n",
    "    dist=svgd_empirical, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "# Mixture\n",
    "svgd_eshd_mix = expected_shd(dist=svgd_mixture, g=data.g)\n",
    "svgd_auroc_mix = threshold_metrics(dist=svgd_mixture, g=data.g)['roc_auc']\n",
    "svgd_negll_mix = neg_ave_log_likelihood(\n",
    "    dist=svgd_mixture, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "print(f\"SVGD Results:\")\n",
    "print(f\"  Empirical - E-SHD: {svgd_eshd_emp:5.2f}, AUROC: {svgd_auroc_emp:5.3f}, NegLL: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Mixture   - E-SHD: {svgd_eshd_mix:5.2f}, AUROC: {svgd_auroc_mix:5.3f}, NegLL: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Time: {svgd_training_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute statistics for ensemble\n",
    "def compute_stats(values):\n",
    "    return {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'median': np.median(values)\n",
    "    }\n",
    "\n",
    "print(\"\\nDEEP ENSEMBLE STATISTICS (20 runs × 1 particle):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for metric_name in ['eshd_empirical', 'auroc_empirical', 'negll_empirical', \n",
    "                   'eshd_mixture', 'auroc_mixture', 'negll_mixture']:\n",
    "    stats = compute_stats(ensemble_metrics[metric_name])\n",
    "    print(f\"{metric_name:15s}: {stats['mean']:6.2f} ± {stats['std']:5.2f} \"\n",
    "          f\"[{stats['min']:5.2f}, {stats['max']:5.2f}] (median: {stats['median']:5.2f})\")\n",
    "\n",
    "training_stats = compute_stats(ensemble_metrics['training_time'])\n",
    "print(f\"{'training_time':15s}: {training_stats['mean']:6.1f} ± {training_stats['std']:5.1f}s \"\n",
    "      f\"(total: {sum(ensemble_metrics['training_time']):.1f}s)\")\n",
    "\n",
    "print(f\"\\nSVGD RESULTS (1 run × 20 particles):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'eshd_empirical':15s}: {svgd_eshd_emp:6.2f}\")\n",
    "print(f\"{'auroc_empirical':15s}: {svgd_auroc_emp:6.2f}\")\n",
    "print(f\"{'negll_empirical':15s}: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"{'eshd_mixture':15s}: {svgd_eshd_mix:6.2f}\")\n",
    "print(f\"{'auroc_mixture':15s}: {svgd_auroc_mix:6.2f}\")\n",
    "print(f\"{'negll_mixture':15s}: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"{'training_time':15s}: {svgd_training_time:6.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare approaches\n",
    "print(\"\\nEMPIRICAL DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "ensemble_mean_eshd_emp = np.mean(ensemble_metrics['eshd_empirical'])\n",
    "ensemble_mean_auroc_emp = np.mean(ensemble_metrics['auroc_empirical'])\n",
    "ensemble_mean_negll_emp = np.mean(ensemble_metrics['negll_empirical'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_emp:5.2f} (± {np.std(ensemble_metrics['eshd_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_emp:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_emp - svgd_eshd_emp:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_emp:5.3f} (± {np.std(ensemble_metrics['auroc_empirical']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_emp:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_emp - svgd_auroc_emp:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_emp:6.2f} (± {np.std(ensemble_metrics['negll_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_emp - svgd_negll_emp:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\nMIXTURE DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "ensemble_mean_eshd_mix = np.mean(ensemble_metrics['eshd_mixture'])\n",
    "ensemble_mean_auroc_mix = np.mean(ensemble_metrics['auroc_mixture'])\n",
    "ensemble_mean_negll_mix = np.mean(ensemble_metrics['negll_mixture'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_mix:5.2f} (± {np.std(ensemble_metrics['eshd_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_mix:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_mix - svgd_eshd_mix:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_mix:5.3f} (± {np.std(ensemble_metrics['auroc_mixture']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_mix:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_mix - svgd_auroc_mix:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_mix:6.2f} (± {np.std(ensemble_metrics['negll_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_mix - svgd_negll_mix:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "better_empirical = []\n",
    "better_mixture = []\n",
    "\n",
    "if ensemble_mean_eshd_emp < svgd_eshd_emp:\n",
    "    better_empirical.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_emp > svgd_auroc_emp:\n",
    "    better_empirical.append(\"AUROC\")\n",
    "if ensemble_mean_negll_emp < svgd_negll_emp:\n",
    "    better_empirical.append(\"NegLL\")\n",
    "\n",
    "if ensemble_mean_eshd_mix < svgd_eshd_mix:\n",
    "    better_mixture.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_mix > svgd_auroc_mix:\n",
    "    better_mixture.append(\"AUROC\")\n",
    "if ensemble_mean_negll_mix < svgd_negll_mix:\n",
    "    better_mixture.append(\"NegLL\")\n",
    "\n",
    "print(f\"Deep Ensemble outperforms SVGD on:\")\n",
    "print(f\"  Empirical distribution: {better_empirical if better_empirical else 'None'}\")\n",
    "print(f\"  Mixture distribution:   {better_mixture if better_mixture else 'None'}\")\n",
    "\n",
    "total_ensemble_time = sum(ensemble_metrics['training_time'])\n",
    "print(f\"\\nComputational efficiency:\")\n",
    "print(f\"  Deep Ensemble total time: {total_ensemble_time:.1f}s\")\n",
    "print(f\"  SVGD time:               {svgd_training_time:.1f}s\")\n",
    "print(f\"  Time ratio (Ensemble/SVGD): {total_ensemble_time/svgd_training_time:.1f}x\")\n",
    "\n",
    "print(f\"\\nThis comparison shows the trade-offs between:\")\n",
    "print(f\"  - Deep Ensemble: Independent optimization, potential for diverse solutions\")\n",
    "print(f\"  - SVGD: Particle interaction, computational efficiency, Bayesian approach\")\n",
    "\n",
    "# Save results for further analysis\n",
    "results_dict = {\n",
    "    'ensemble_results': ensemble_results,\n",
    "    'ensemble_metrics': ensemble_metrics,\n",
    "    'svgd_results': {\n",
    "        'eshd_empirical': svgd_eshd_emp,\n",
    "        'auroc_empirical': svgd_auroc_emp,\n",
    "        'negll_empirical': svgd_negll_emp,\n",
    "        'eshd_mixture': svgd_eshd_mix,\n",
    "        'auroc_mixture': svgd_auroc_mix,\n",
    "        'negll_mixture': svgd_negll_mix,\n",
    "        'training_time': svgd_training_time\n",
    "    },\n",
    "    'ground_truth_edges': np.sum(data.g),\n",
    "    'experiment_params': {\n",
    "        'n_ensemble_runs': n_ensemble_runs,\n",
    "        'n_particles_svgd': n_particles_svgd,\n",
    "        'n_steps': n_steps,\n",
    "        'n_vars': data.x.shape[1],\n",
    "        'n_train_samples': data.x.shape[0],\n",
    "        'n_test_samples': data.x_ho.shape[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nResults stored in 'results_dict' variable for further analysis.\")\n",
    "print(f\"Individual ensemble runs available in 'ensemble_results' list.\")\n",
    "print(f\"Ensemble aggregated metrics available in 'ensemble_metrics' dict.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETED\")\n",
    "print(\"=\"*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import jax\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "from dibs.target import make_nonlinear_gaussian_model\n",
    "from dibs.inference import JointDiBS\n",
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "main_key = random.PRNGKey(42)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# Generate ground truth nonlinear Gaussian model\n",
    "print(\"Generating ground truth nonlinear Gaussian model...\")\n",
    "key, subk = random.split(main_key)\n",
    "data, graph_model, likelihood_model = make_nonlinear_gaussian_model(\n",
    "    key=subk, \n",
    "    n_vars=20, \n",
    "    graph_prior_str=\"sf\"\n",
    ")\n",
    "\n",
    "print(f\"Ground truth graph has {np.sum(data.g)} edges\")\n",
    "print(\"Visualizing ground truth...\")\n",
    "try:\n",
    "    visualize_ground_truth(data.g)\n",
    "except:\n",
    "    print(\"Visualization skipped (may not work in all environments)\")\n",
    "\n",
    "# Experiment parameters\n",
    "n_ensemble_runs = 20\n",
    "n_particles_svgd = 20\n",
    "n_steps = 2000\n",
    "callback_every = 500\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SETUP\")\n",
    "print(f\"  Deep Ensemble: {n_ensemble_runs} runs × 1 particle each\")\n",
    "print(f\"  SVGD: 1 run × {n_particles_svgd} particles\")\n",
    "print(f\"  Training steps: {n_steps}\")\n",
    "print(f\"  Variables: {data.x.shape[1]}\")\n",
    "print(f\"  Training samples: {data.x.shape[0]}\")\n",
    "print(f\"  Test samples: {data.x_ho.shape[0]}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Storage for results\n",
    "ensemble_results = []\n",
    "ensemble_metrics = {\n",
    "    'eshd_empirical': [],\n",
    "    'auroc_empirical': [],\n",
    "    'negll_empirical': [],\n",
    "    'eshd_mixture': [],\n",
    "    'auroc_mixture': [],\n",
    "    'negll_mixture': [],\n",
    "    'training_time': []\n",
    "}\n",
    "\n",
    "# NEW: Storage for true ensemble (combining all samples)\n",
    "all_ensemble_gs = []\n",
    "all_ensemble_thetas = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEEP ENSEMBLE APPROACH (20 runs × 1 particle)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Deep Ensemble: 20 runs with 1 particle each\n",
    "for run_idx in range(n_ensemble_runs):\n",
    "    print(f\"\\nRun {run_idx + 1}/{n_ensemble_runs}\")\n",
    "    \n",
    "    # Use different seed for each run\n",
    "    key, subk = random.split(key)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create DiBS instance\n",
    "    dibs = JointDiBS(\n",
    "        x=data.x, \n",
    "        interv_mask=None, \n",
    "        graph_model=graph_model, \n",
    "        likelihood_model=likelihood_model\n",
    "    )\n",
    "    \n",
    "    # Sample with 1 particle\n",
    "    gs, thetas = dibs.sample(\n",
    "        key=subk, \n",
    "        n_particles=1, \n",
    "        steps=n_steps, \n",
    "        callback_every=callback_every\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Get distributions\n",
    "    dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "    dibs_mixture = dibs.get_mixture(gs, thetas)\n",
    "    \n",
    "    # Compute metrics\n",
    "    # Empirical\n",
    "    eshd_emp = expected_shd(dist=dibs_empirical, g=data.g)\n",
    "    auroc_emp = threshold_metrics(dist=dibs_empirical, g=data.g)['roc_auc']\n",
    "    negll_emp = neg_ave_log_likelihood(\n",
    "        dist=dibs_empirical, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Mixture\n",
    "    eshd_mix = expected_shd(dist=dibs_mixture, g=data.g)\n",
    "    auroc_mix = threshold_metrics(dist=dibs_mixture, g=data.g)['roc_auc']\n",
    "    negll_mix = neg_ave_log_likelihood(\n",
    "        dist=dibs_mixture, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    run_result = {\n",
    "        'run_idx': run_idx,\n",
    "        'eshd_empirical': eshd_emp,\n",
    "        'auroc_empirical': auroc_emp,\n",
    "        'negll_empirical': negll_emp,\n",
    "        'eshd_mixture': eshd_mix,\n",
    "        'auroc_mixture': auroc_mix,\n",
    "        'negll_mixture': negll_mix,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    ensemble_results.append(run_result)\n",
    "    \n",
    "    # Also store in lists for easy aggregation\n",
    "    ensemble_metrics['eshd_empirical'].append(eshd_emp)\n",
    "    ensemble_metrics['auroc_empirical'].append(auroc_emp)\n",
    "    ensemble_metrics['negll_empirical'].append(negll_emp)\n",
    "    ensemble_metrics['eshd_mixture'].append(eshd_mix)\n",
    "    ensemble_metrics['auroc_mixture'].append(auroc_mix)\n",
    "    ensemble_metrics['negll_mixture'].append(negll_mix)\n",
    "    ensemble_metrics['training_time'].append(training_time)\n",
    "    \n",
    "    # NEW: Store samples for true ensemble\n",
    "    all_ensemble_gs.append(gs)\n",
    "    all_ensemble_thetas.append(thetas)\n",
    "    \n",
    "    print(f\"  Empirical - E-SHD: {eshd_emp:5.2f}, AUROC: {auroc_emp:5.3f}, NegLL: {negll_emp:6.2f}\")\n",
    "    print(f\"  Mixture   - E-SHD: {eshd_mix:5.2f}, AUROC: {auroc_mix:5.3f}, NegLL: {negll_mix:6.2f}\")\n",
    "    print(f\"  Time: {training_time:.1f}s\")\n",
    "\n",
    "# NEW: Compute TRUE ENSEMBLE by combining ALL samples from all runs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRUE DEEP ENSEMBLE (combining all 20 samples)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine all graphs and parameters from all runs into single arrays\n",
    "combined_gs = np.concatenate(all_ensemble_gs, axis=0)  # [20, d, d] \n",
    "combined_thetas = jax.tree_map(lambda *arrays: np.concatenate(arrays, axis=0), *all_ensemble_thetas)\n",
    "\n",
    "print(f\"Combined ensemble contains {combined_gs.shape[0]} total samples\")\n",
    "\n",
    "# Create a single DiBS instance to compute distributions (any will work since we're just using the method)\n",
    "dibs_for_ensemble = JointDiBS(\n",
    "    x=data.x, \n",
    "    interv_mask=None, \n",
    "    graph_model=graph_model, \n",
    "    likelihood_model=likelihood_model\n",
    ")\n",
    "\n",
    "# Get true ensemble distributions\n",
    "true_ensemble_empirical = dibs_for_ensemble.get_empirical(combined_gs, combined_thetas)\n",
    "true_ensemble_mixture = dibs_for_ensemble.get_mixture(combined_gs, combined_thetas)\n",
    "\n",
    "# Compute metrics on true ensemble\n",
    "true_eshd_emp = expected_shd(dist=true_ensemble_empirical, g=data.g)\n",
    "true_auroc_emp = threshold_metrics(dist=true_ensemble_empirical, g=data.g)['roc_auc']\n",
    "true_negll_emp = neg_ave_log_likelihood(\n",
    "    dist=true_ensemble_empirical, \n",
    "    eltwise_log_likelihood=dibs_for_ensemble.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "true_eshd_mix = expected_shd(dist=true_ensemble_mixture, g=data.g)\n",
    "true_auroc_mix = threshold_metrics(dist=true_ensemble_mixture, g=data.g)['roc_auc']\n",
    "true_negll_mix = neg_ave_log_likelihood(\n",
    "    dist=true_ensemble_mixture, \n",
    "    eltwise_log_likelihood=dibs_for_ensemble.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "print(f\"TRUE ENSEMBLE Results:\")\n",
    "print(f\"  Empirical - E-SHD: {true_eshd_emp:5.2f}, AUROC: {true_auroc_emp:5.3f}, NegLL: {true_negll_emp:6.2f}\")\n",
    "print(f\"  Mixture   - E-SHD: {true_eshd_mix:5.2f}, AUROC: {true_auroc_mix:5.3f}, NegLL: {true_negll_mix:6.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVGD APPROACH (1 run × 20 particles)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SVGD: 1 run with 20 particles\n",
    "key, subk = random.split(key)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dibs_svgd = JointDiBS(\n",
    "    x=data.x, \n",
    "    interv_mask=None, \n",
    "    graph_model=graph_model, \n",
    "    likelihood_model=likelihood_model\n",
    ")\n",
    "\n",
    "gs_svgd, thetas_svgd = dibs_svgd.sample(\n",
    "    key=subk, \n",
    "    n_particles=n_particles_svgd, \n",
    "    steps=n_steps, \n",
    "    callback_every=callback_every\n",
    ")\n",
    "\n",
    "svgd_training_time = time.time() - start_time\n",
    "\n",
    "# Get distributions\n",
    "svgd_empirical = dibs_svgd.get_empirical(gs_svgd, thetas_svgd)\n",
    "svgd_mixture = dibs_svgd.get_mixture(gs_svgd, thetas_svgd)\n",
    "\n",
    "# Compute metrics\n",
    "# Empirical\n",
    "svgd_eshd_emp = expected_shd(dist=svgd_empirical, g=data.g)\n",
    "svgd_auroc_emp = threshold_metrics(dist=svgd_empirical, g=data.g)['roc_auc']\n",
    "svgd_negll_emp = neg_ave_log_likelihood(\n",
    "    dist=svgd_empirical, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "# Mixture\n",
    "svgd_eshd_mix = expected_shd(dist=svgd_mixture, g=data.g)\n",
    "svgd_auroc_mix = threshold_metrics(dist=svgd_mixture, g=data.g)['roc_auc']\n",
    "svgd_negll_mix = neg_ave_log_likelihood(\n",
    "    dist=svgd_mixture, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "print(f\"SVGD Results:\")\n",
    "print(f\"  Empirical - E-SHD: {svgd_eshd_emp:5.2f}, AUROC: {svgd_auroc_emp:5.3f}, NegLL: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Mixture   - E-SHD: {svgd_eshd_mix:5.2f}, AUROC: {svgd_auroc_mix:5.3f}, NegLL: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Time: {svgd_training_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute statistics for ensemble\n",
    "def compute_stats(values):\n",
    "    return {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'median': np.median(values)\n",
    "    }\n",
    "\n",
    "print(\"\\nDEEP ENSEMBLE STATISTICS - AVERAGE OF INDIVIDUALS (20 runs × 1 particle):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for metric_name in ['eshd_empirical', 'auroc_empirical', 'negll_empirical', \n",
    "                   'eshd_mixture', 'auroc_mixture', 'negll_mixture']:\n",
    "    stats = compute_stats(ensemble_metrics[metric_name])\n",
    "    print(f\"{metric_name:15s}: {stats['mean']:6.2f} ± {stats['std']:5.2f} \"\n",
    "          f\"[{stats['min']:5.2f}, {stats['max']:5.2f}] (median: {stats['median']:5.2f})\")\n",
    "\n",
    "training_stats = compute_stats(ensemble_metrics['training_time'])\n",
    "print(f\"{'training_time':15s}: {training_stats['mean']:6.1f} ± {training_stats['std']:5.1f}s \"\n",
    "      f\"(total: {sum(ensemble_metrics['training_time']):.1f}s)\")\n",
    "\n",
    "print(f\"\\nTRUE DEEP ENSEMBLE STATISTICS (combined 20 samples):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'eshd_empirical':15s}: {true_eshd_emp:6.2f}\")\n",
    "print(f\"{'auroc_empirical':15s}: {true_auroc_emp:6.2f}\")\n",
    "print(f\"{'negll_empirical':15s}: {true_negll_emp:6.2f}\")\n",
    "print(f\"{'eshd_mixture':15s}: {true_eshd_mix:6.2f}\")\n",
    "print(f\"{'auroc_mixture':15s}: {true_auroc_mix:6.2f}\")\n",
    "print(f\"{'negll_mixture':15s}: {true_negll_mix:6.2f}\")\n",
    "\n",
    "print(f\"\\nSVGD RESULTS (1 run × 20 particles):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'eshd_empirical':15s}: {svgd_eshd_emp:6.2f}\")\n",
    "print(f\"{'auroc_empirical':15s}: {svgd_auroc_emp:6.2f}\")\n",
    "print(f\"{'negll_empirical':15s}: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"{'eshd_mixture':15s}: {svgd_eshd_mix:6.2f}\")\n",
    "print(f\"{'auroc_mixture':15s}: {svgd_auroc_mix:6.2f}\")\n",
    "print(f\"{'negll_mixture':15s}: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"{'training_time':15s}: {svgd_training_time:6.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare approaches\n",
    "print(\"\\nEMPIRICAL DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"(A) AVERAGE-OF-INDIVIDUALS vs SVGD:\")\n",
    "ensemble_mean_eshd_emp = np.mean(ensemble_metrics['eshd_empirical'])\n",
    "ensemble_mean_auroc_emp = np.mean(ensemble_metrics['auroc_empirical'])\n",
    "ensemble_mean_negll_emp = np.mean(ensemble_metrics['negll_empirical'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_emp:5.2f} (± {np.std(ensemble_metrics['eshd_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_emp:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_emp - svgd_eshd_emp:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_emp:5.3f} (± {np.std(ensemble_metrics['auroc_empirical']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_emp:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_emp - svgd_auroc_emp:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_emp:6.2f} (± {np.std(ensemble_metrics['negll_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_emp - svgd_negll_emp:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\n(B) TRUE-ENSEMBLE vs SVGD:\")\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  True Ensemble: {true_eshd_emp:5.2f}\")\n",
    "print(f\"  SVGD:          {svgd_eshd_emp:5.2f}\")\n",
    "print(f\"  Difference:    {true_eshd_emp - svgd_eshd_emp:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  True Ensemble: {true_auroc_emp:5.3f}\")\n",
    "print(f\"  SVGD:          {svgd_auroc_emp:5.3f}\")\n",
    "print(f\"  Difference:    {true_auroc_emp - svgd_auroc_emp:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  True Ensemble: {true_negll_emp:6.2f}\")\n",
    "print(f\"  SVGD:          {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Difference:    {true_negll_emp - svgd_negll_emp:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\nMIXTURE DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"(A) AVERAGE-OF-INDIVIDUALS vs SVGD:\")\n",
    "ensemble_mean_eshd_mix = np.mean(ensemble_metrics['eshd_mixture'])\n",
    "ensemble_mean_auroc_mix = np.mean(ensemble_metrics['auroc_mixture'])\n",
    "ensemble_mean_negll_mix = np.mean(ensemble_metrics['negll_mixture'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_mix:5.2f} (± {np.std(ensemble_metrics['eshd_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_mix:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_mix - svgd_eshd_mix:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_mix:5.3f} (± {np.std(ensemble_metrics['auroc_mixture']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_mix:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_mix - svgd_auroc_mix:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_mix:6.2f} (± {np.std(ensemble_metrics['negll_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_mix - svgd_negll_mix:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\n(B) TRUE-ENSEMBLE vs SVGD:\")\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  True Ensemble: {true_eshd_mix:5.2f}\")\n",
    "print(f\"  SVGD:          {svgd_eshd_mix:5.2f}\")\n",
    "print(f\"  Difference:    {true_eshd_mix - svgd_eshd_mix:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  True Ensemble: {true_auroc_mix:5.3f}\")\n",
    "print(f\"  SVGD:          {svgd_auroc_mix:5.3f}\")\n",
    "print(f\"  Difference:    {true_auroc_mix - svgd_auroc_mix:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  True Ensemble: {true_negll_mix:6.2f}\")\n",
    "print(f\"  SVGD:          {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Difference:    {true_negll_mix - svgd_negll_mix:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average-of-individuals comparison\n",
    "better_empirical_avg = []\n",
    "better_mixture_avg = []\n",
    "\n",
    "if ensemble_mean_eshd_emp < svgd_eshd_emp:\n",
    "    better_empirical_avg.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_emp > svgd_auroc_emp:\n",
    "    better_empirical_avg.append(\"AUROC\")\n",
    "if ensemble_mean_negll_emp < svgd_negll_emp:\n",
    "    better_empirical_avg.append(\"NegLL\")\n",
    "\n",
    "if ensemble_mean_eshd_mix < svgd_eshd_mix:\n",
    "    better_mixture_avg.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_mix > svgd_auroc_mix:\n",
    "    better_mixture_avg.append(\"AUROC\")\n",
    "if ensemble_mean_negll_mix < svgd_negll_mix:\n",
    "    better_mixture_avg.append(\"NegLL\")\n",
    "\n",
    "# True ensemble comparison\n",
    "better_empirical_true = []\n",
    "better_mixture_true = []\n",
    "\n",
    "if true_eshd_emp < svgd_eshd_emp:\n",
    "    better_empirical_true.append(\"E-SHD\")\n",
    "if true_auroc_emp > svgd_auroc_emp:\n",
    "    better_empirical_true.append(\"AUROC\")\n",
    "if true_negll_emp < svgd_negll_emp:\n",
    "    better_empirical_true.append(\"NegLL\")\n",
    "\n",
    "if true_eshd_mix < svgd_eshd_mix:\n",
    "    better_mixture_true.append(\"E-SHD\")\n",
    "if true_auroc_mix > svgd_auroc_mix:\n",
    "    better_mixture_true.append(\"AUROC\")\n",
    "if true_negll_mix < svgd_negll_mix:\n",
    "    better_mixture_true.append(\"NegLL\")\n",
    "\n",
    "print(f\"AVERAGE-OF-INDIVIDUALS Deep Ensemble outperforms SVGD on:\")\n",
    "print(f\"  Empirical distribution: {better_empirical_avg if better_empirical_avg else 'None'}\")\n",
    "print(f\"  Mixture distribution:   {better_mixture_avg if better_mixture_avg else 'None'}\")\n",
    "\n",
    "print(f\"\\nTRUE Deep Ensemble outperforms SVGD on:\")\n",
    "print(f\"  Empirical distribution: {better_empirical_true if better_empirical_true else 'None'}\")\n",
    "print(f\"  Mixture distribution:   {better_mixture_true if better_mixture_true else 'None'}\")\n",
    "\n",
    "total_ensemble_time = sum(ensemble_metrics['training_time'])\n",
    "print(f\"\\nComputational efficiency:\")\n",
    "print(f\"  Deep Ensemble total time: {total_ensemble_time:.1f}s\")\n",
    "print(f\"  SVGD time:               {svgd_training_time:.1f}s\")\n",
    "print(f\"  Time ratio (Ensemble/SVGD): {total_ensemble_time/svgd_training_time:.1f}x\")\n",
    "\n",
    "print(f\"\\nIMPORTANT: This comparison demonstrates two different ways to use deep ensembles:\")\n",
    "print(f\"  - AVERAGE-OF-INDIVIDUALS: Average the performance metrics across runs\")\n",
    "print(f\"  - TRUE ENSEMBLE: Combine all samples into one distribution, then evaluate\")\n",
    "print(f\"  - SVGD: Particle interaction for Bayesian inference\")\n",
    "print(f\"\\nThe TRUE ENSEMBLE approach is the proper way to evaluate ensemble methods!\")\n",
    "print(f\"Averaging individual performances != Performance of the ensemble!\")\n",
    "\n",
    "# Save results for further analysis\n",
    "results_dict = {\n",
    "    'ensemble_results': ensemble_results,\n",
    "    'ensemble_metrics': ensemble_metrics,\n",
    "    'true_ensemble_results': {\n",
    "        'eshd_empirical': true_eshd_emp,\n",
    "        'auroc_empirical': true_auroc_emp,\n",
    "        'negll_empirical': true_negll_emp,\n",
    "        'eshd_mixture': true_eshd_mix,\n",
    "        'auroc_mixture': true_auroc_mix,\n",
    "        'negll_mixture': true_negll_mix,\n",
    "        'combined_samples': combined_gs.shape[0]\n",
    "    },\n",
    "    'svgd_results': {\n",
    "        'eshd_empirical': svgd_eshd_emp,\n",
    "        'auroc_empirical': svgd_auroc_emp,\n",
    "        'negll_empirical': svgd_negll_emp,\n",
    "        'eshd_mixture': svgd_eshd_mix,\n",
    "        'auroc_mixture': svgd_auroc_mix,\n",
    "        'negll_mixture': svgd_negll_mix,\n",
    "        'training_time': svgd_training_time\n",
    "    },\n",
    "    'ground_truth_edges': np.sum(data.g),\n",
    "    'experiment_params': {\n",
    "        'n_ensemble_runs': n_ensemble_runs,\n",
    "        'n_particles_svgd': n_particles_svgd,\n",
    "        'n_steps': n_steps,\n",
    "        'n_vars': data.x.shape[1],\n",
    "        'n_train_samples': data.x.shape[0],\n",
    "        'n_test_samples': data.x_ho.shape[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nResults stored in 'results_dict' variable for further analysis.\")\n",
    "print(f\"Individual ensemble runs available in 'ensemble_results' list.\")\n",
    "print(f\"Ensemble aggregated metrics available in 'ensemble_metrics' dict.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETED\")\n",
    "print(\"=\"*60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple Deep Ensemble vs SVGD Comparison\n",
    "Based on dibs_joint_colab.ipynb approach\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from dibs.target import make_nonlinear_gaussian_model\n",
    "from dibs.inference import JointDiBS\n",
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "# Setup\n",
    "key = random.PRNGKey(42)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# Generate ground truth\n",
    "key, subk = random.split(key)\n",
    "data, graph_model, likelihood_model = make_nonlinear_gaussian_model(\n",
    "    key=subk, n_vars=20, graph_prior_str=\"sf\"\n",
    ")\n",
    "print(f\"Ground truth graph has {np.sum(data.g)} edges\")\n",
    "\n",
    "# =============================================================================\n",
    "# SVGD (20 particles)\n",
    "# =============================================================================\n",
    "print(\"\\n=== SVGD (20 particles) ===\")\n",
    "key, subk = random.split(key)\n",
    "dibs = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "\n",
    "start_time = time.time()\n",
    "gs_svgd, thetas_svgd = dibs.sample(key=subk, n_particles=20, steps=2000)\n",
    "svgd_time = time.time() - start_time\n",
    "\n",
    "svgd_empirical = dibs.get_empirical(gs_svgd, thetas_svgd)\n",
    "svgd_mixture = dibs.get_mixture(gs_svgd, thetas_svgd)\n",
    "\n",
    "# =============================================================================\n",
    "# Deep Ensemble (20 × 1 particle)\n",
    "# =============================================================================\n",
    "print(\"\\n=== Deep Ensemble (20 × 1 particle) ===\")\n",
    "ensemble_gs = []\n",
    "ensemble_thetas = []\n",
    "ensemble_empiricals = []\n",
    "ensemble_mixtures = []\n",
    "\n",
    "ensemble_start = time.time()\n",
    "for i in range(20):\n",
    "    print(f\"Run {i+1}/20\", end=\" \")\n",
    "    key, subk = random.split(key)\n",
    "    dibs_single = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "    \n",
    "    gs, thetas = dibs_single.sample(key=subk, n_particles=1, steps=2000)\n",
    "    empirical = dibs_single.get_empirical(gs, thetas)\n",
    "    mixture = dibs_single.get_mixture(gs, thetas)\n",
    "    \n",
    "    ensemble_gs.append(gs)\n",
    "    ensemble_thetas.append(thetas)\n",
    "    ensemble_empiricals.append(empirical)\n",
    "    ensemble_mixtures.append(mixture)\n",
    "    print(\"✓\")\n",
    "\n",
    "ensemble_time = time.time() - ensemble_start\n",
    "\n",
    "# Combine all samples for true ensemble\n",
    "combined_gs = np.concatenate(ensemble_gs, axis=0)\n",
    "combined_thetas = jax.tree_map(lambda *arrays: np.concatenate(arrays, axis=0), *ensemble_thetas)\n",
    "\n",
    "# Create true ensemble distributions\n",
    "dibs_combined = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "true_ensemble_empirical = dibs_combined.get_empirical(combined_gs, combined_thetas)\n",
    "true_ensemble_mixture = dibs_combined.get_mixture(combined_gs, combined_thetas)\n",
    "\n",
    "# =============================================================================\n",
    "# Evaluation\n",
    "# =============================================================================\n",
    "def compute_metrics(dist, name, dibs_instance):\n",
    "    eshd = expected_shd(dist=dist, g=data.g)\n",
    "    auroc = threshold_metrics(dist=dist, g=data.g)['roc_auc']\n",
    "    negll = neg_ave_log_likelihood(dist=dist, eltwise_log_likelihood=dibs_instance.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    print(f'{name:30s} | E-SHD: {eshd:4.1f}  AUROC: {auroc:5.3f}  NegLL: {negll:6.1f}')\n",
    "    return {'eshd': eshd, 'auroc': auroc, 'negll': negll}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# SVGD results\n",
    "svgd_emp_metrics = compute_metrics(svgd_empirical, 'SVGD Empirical', dibs)\n",
    "svgd_mix_metrics = compute_metrics(svgd_mixture, 'SVGD Mixture', dibs)\n",
    "\n",
    "# True ensemble results\n",
    "true_emp_metrics = compute_metrics(true_ensemble_empirical, 'True Ensemble Empirical', dibs_combined)\n",
    "true_mix_metrics = compute_metrics(true_ensemble_mixture, 'True Ensemble Mixture', dibs_combined)\n",
    "\n",
    "# Average of individuals (like in notebook)\n",
    "def average_metric(metric_fn, dists):\n",
    "    return sum(metric_fn(dist=dist, g=data.g) for dist in dists) / len(dists)\n",
    "\n",
    "def average_negll(dists, dibs_instance):\n",
    "    return sum(neg_ave_log_likelihood(dist=dist, eltwise_log_likelihood=dibs_instance.eltwise_log_likelihood_observ, x=data.x_ho) \n",
    "               for dist in dists) / len(dists)\n",
    "\n",
    "avg_emp_eshd = average_metric(expected_shd, ensemble_empiricals)\n",
    "avg_emp_auroc = average_metric(lambda **kw: threshold_metrics(**kw)['roc_auc'], ensemble_empiricals)\n",
    "avg_emp_negll = average_negll(ensemble_empiricals, dibs_single)\n",
    "\n",
    "avg_mix_eshd = average_metric(expected_shd, ensemble_mixtures)\n",
    "avg_mix_auroc = average_metric(lambda **kw: threshold_metrics(**kw)['roc_auc'], ensemble_mixtures)\n",
    "avg_mix_negll = average_negll(ensemble_mixtures, dibs_single)\n",
    "\n",
    "print(f'{\"Average Empirical\":30s} | E-SHD: {avg_emp_eshd:4.1f}  AUROC: {avg_emp_auroc:5.3f}  NegLL: {avg_emp_negll:6.1f}')\n",
    "print(f'{\"Average Mixture\":30s} | E-SHD: {avg_mix_eshd:4.1f}  AUROC: {avg_mix_auroc:5.3f}  NegLL: {avg_mix_negll:6.1f}')\n",
    "\n",
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Computation time:\")\n",
    "print(f\"  SVGD (20 particles):           {svgd_time:5.1f}s\")\n",
    "print(f\"  Deep Ensemble (20 × 1):        {ensemble_time:5.1f}s\")\n",
    "print(f\"  Speedup (SVGD vs Ensemble):    {ensemble_time/svgd_time:5.1f}x\")\n",
    "\n",
    "print(f\"\\nEmpirical distribution (E-SHD, lower is better):\")\n",
    "print(f\"  SVGD:               {svgd_emp_metrics['eshd']:5.1f}\")\n",
    "print(f\"  True Ensemble:      {true_emp_metrics['eshd']:5.1f}\")\n",
    "print(f\"  Average Individual: {avg_emp_eshd:5.1f}\")\n",
    "\n",
    "print(f\"\\nEmpirical distribution (AUROC, higher is better):\")\n",
    "print(f\"  SVGD:               {svgd_emp_metrics['auroc']:5.3f}\")\n",
    "print(f\"  True Ensemble:      {true_emp_metrics['auroc']:5.3f}\")\n",
    "print(f\"  Average Individual: {avg_emp_auroc:5.3f}\")\n",
    "\n",
    "print(f\"\\nKey insight: True Ensemble != Average of Individuals\")\n",
    "print(f\"Total samples: SVGD=20, True Ensemble=20, Individuals=20×1\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
