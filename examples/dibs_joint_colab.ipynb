{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e361100",
   "metadata": {},
   "source": [
    "## Example: Joint inference of $p(G, \\Theta | D)$ for Gaussian Bayes nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d0dfc",
   "metadata": {},
   "source": [
    "Setup for Google Colab. Selecting the **GPU** runtime available in Google colab will make inference significantly faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet dibs-lib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b52aa",
   "metadata": {},
   "source": [
    "DiBS translates the task of inferring the posterior over Bayesian networks into an inference problem over the continuous latent variable $Z$. This is achieved by modeling the directed acyclic graph $G$ of the Bayesian network using the generative model $p(G | Z)$. The prior $p(Z)$ enforces the acyclicity of $G$.\n",
    "Ultimately, this allows us to infer $p(G, \\Theta | D)$ (and $p(G | D)$) using off-the-shelf inference methods such as Stein Variational gradient descent (SVGD) (Liu and Wang, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8370c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.random as random\n",
    "key = random.PRNGKey(123)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5e4a",
   "metadata": {},
   "source": [
    "### Generate synthetic ground truth Bayesian network and BN model for inference\n",
    "\n",
    "`data` contains information about and observations sampled from a synthetic, ground truth causal model with `n_vars` variables. By default, the conditional distributions are linear Gaussian. The random graph model is set by `graph_prior_str`, where `er` denotes Erdos-Renyi and `sf` scale-free graphs. \n",
    "\n",
    "`graph_model` defines prior $p(G)$ and `likelihood_model` defines likelihood $p(x, \\Theta| G ) = p(\\Theta| G )p(x | G, \\Theta )$ of the BN model for which DiBS will infer the posterior.\n",
    "\n",
    "**For posterior inference of nonlinear Gaussian networks parameterized by fully-connected neural networks, use the function `make_nonlinear_gaussian_model`.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98551a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dibs.target import make_linear_gaussian_model, make_nonlinear_gaussian_model\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "key, subk = random.split(key)\n",
    "data, graph_model, likelihood_model = make_linear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "# data, graph_model, likelihood_model = make_nonlinear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "\n",
    "visualize_ground_truth(data.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e46b",
   "metadata": {},
   "source": [
    "### DiBS with SVGD\n",
    "\n",
    "Infer $p(G, \\Theta | D)$ under the prior and conditional distributions defined by the model.\n",
    "The below visualization shows the *matrix of edge probabilities* $G_\\alpha(Z^{(k)})$ implied by each transported latent particle (i.e., sample) $Z^{(k)}$ during the iterations of SVGD with DiBS. Refer to the paper for further details.\n",
    "\n",
    "To explicitly perform posterior inference of $p(G | D)$ using a closed-form marginal likelihood $p(D | G)$, use the separate, analogous class `MarginalDiBS` as demonstrated in the example notebook `dibs_marginal.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ece4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.inference import JointDiBS\n",
    "\n",
    "dibs = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "key, subk = random.split(key)\n",
    "gs, thetas = dibs.sample(key=subk, n_particles=20, steps=2000, callback_every=100, callback=dibs.visualize_callback())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dc8cc",
   "metadata": {},
   "source": [
    "### Evaluate on held-out data\n",
    "\n",
    "Form the empirical (i.e., weighted by counts) and mixture distributions (i.e., weighted by unnormalized posterior probabilities, denoted DiBS+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "dibs_mixture = dibs.get_mixture(gs, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e275383",
   "metadata": {},
   "source": [
    "Compute some evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3dc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "\n",
    "for descr, dist in [('DiBS ', dibs_empirical), ('DiBS+', dibs_mixture)]:\n",
    "    \n",
    "    eshd = expected_shd(dist=dist, g=data.g)        \n",
    "    auroc = threshold_metrics(dist=dist, g=data.g)['roc_auc']\n",
    "    negll = neg_ave_log_likelihood(dist=dist, eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    \n",
    "    print(f'{descr} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}    neg. LL {negll:5.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9f4eb",
   "metadata": {},
   "source": [
    "## Deep Ensemble Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dibs.target import make_nonlinear_gaussian_model\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key, subk = random.split(key)\n",
    "data, graph_model, likelihood_model = make_nonlinear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "visualize_ground_truth(data.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibs = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "key, subk = random.split(key)\n",
    "gs_20, thetas_20 = dibs.sample(key=subk, n_particles=20, steps=2000, callback_every=100, callback=dibs.visualize_callback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df596abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibs_empirical_20 = dibs.get_empirical(gs_20, thetas_20)\n",
    "dibs_mixture_20 = dibs.get_mixture(gs_20, thetas_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_empiricals = []\n",
    "single_mixtures = []\n",
    "key_de = key\n",
    "for i in range(20):\n",
    "    key_de, subk = random.split(key_de)\n",
    "    dibs_single = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "    gs, thetas = dibs_single.sample(key=subk, n_particles=1, steps=2000)  # No visualization callback for speed\n",
    "    empirical = dibs_single.get_empirical(gs, thetas)\n",
    "    mixture = dibs_single.get_mixture(gs, thetas)\n",
    "    single_empiricals.append(empirical)\n",
    "    single_mixtures.append(mixture)\n",
    "\n",
    "# For aggregate deep ensemble: uniform average over the individual distributions\n",
    "# (custom implementation; assumes distributions have a .prob method or similar for averaging)\n",
    "def average_dist(dists, g_true):\n",
    "    def avg_metric(metric_fn):\n",
    "        return sum(metric_fn(dist=dist, g=g_true) for dist in dists) / len(dists)\n",
    "    return avg_metric\n",
    "\n",
    "deep_empirical_avg = average_dist(single_empiricals, data.g)\n",
    "deep_mixture_avg = average_dist(single_mixtures, data.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "\n",
    "# Helper to compute metrics for a dist (or averaged dist)\n",
    "def compute_metrics(dist, name):\n",
    "    if callable(dist):  # For averaged dists\n",
    "        eshd = dist(expected_shd)\n",
    "        auroc = dist(lambda **kw: threshold_metrics(**kw)['roc_auc'])\n",
    "        negll = dist(lambda **kw: neg_ave_log_likelihood(eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho, **kw))\n",
    "    else:  # For standard dists\n",
    "        eshd = expected_shd(dist=dist, g=data.g)\n",
    "        auroc = threshold_metrics(dist=dist, g=data.g)['roc_auc']\n",
    "        negll = neg_ave_log_likelihood(dist=dist, eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    print(f'{name} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}    neg. LL {negll:5.2f}')\n",
    "\n",
    "compute_metrics(dibs_empirical_20, 'SVGD empirical (20 particles)')\n",
    "compute_metrics(dibs_mixture_20, 'SVGD mixture (20 particles)')\n",
    "compute_metrics(deep_empirical_avg, 'Deep ensemble empirical avg (20 x 1-particle)')\n",
    "compute_metrics(deep_mixture_avg, 'Deep ensemble mixture avg (20 x 1-particle)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba78925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svgd_per_metrics = []\n",
    "for i in range(20):\n",
    "    particle_dist = dibs.get_empirical(gs_20[i:i+1], thetas_20[i:i+1])  # Empirical for each particle\n",
    "    eshd = expected_shd(dist=particle_dist, g=data.g)\n",
    "    auroc = threshold_metrics(dist=particle_dist, g=data.g)['roc_auc']\n",
    "    negll = neg_ave_log_likelihood(dist=particle_dist, eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    svgd_per_metrics.append({'eshd': float(eshd), 'auroc': float(auroc), 'negll': float(negll)})\n",
    "\n",
    "svgd_per_metrics  # Output the list for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946af977",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_empirical_per_metrics = []\n",
    "deep_mixture_per_metrics = []\n",
    "for i in range(20):\n",
    "    emp_dist = single_empiricals[i]\n",
    "    mix_dist = single_mixtures[i]\n",
    "    \n",
    "    eshd_emp = expected_shd(dist=emp_dist, g=data.g)\n",
    "    auroc_emp = threshold_metrics(dist=emp_dist, g=data.g)['roc_auc']\n",
    "    negll_emp = neg_ave_log_likelihood(dist=emp_dist, eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    deep_empirical_per_metrics.append({'eshd': float(eshd_emp), 'auroc': float(auroc_emp), 'negll': float(negll_emp)})\n",
    "    \n",
    "    eshd_mix = expected_shd(dist=mix_dist, g=data.g)\n",
    "    auroc_mix = threshold_metrics(dist=mix_dist, g=data.g)['roc_auc']\n",
    "    negll_mix = neg_ave_log_likelihood(dist=mix_dist, eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    deep_mixture_per_metrics.append({'eshd': float(eshd_mix), 'auroc': float(auroc_mix), 'negll': float(negll_mix)})\n",
    "\n",
    "deep_empirical_per_metrics  # Output for inspection\n",
    "deep_mixture_per_metrics  # Output for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd396de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_key = random.PRNGKey(42)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# Generate ground truth nonlinear Gaussian model\n",
    "print(\"Generating ground truth nonlinear Gaussian model...\")\n",
    "key, subk = random.split(main_key)\n",
    "data, graph_model, likelihood_model = make_nonlinear_gaussian_model(\n",
    "    key=subk, \n",
    "    n_vars=20, \n",
    "    graph_prior_str=\"sf\"\n",
    ")\n",
    "\n",
    "print(f\"Ground truth graph has {np.sum(data.g)} edges\")\n",
    "print(\"Visualizing ground truth...\")\n",
    "try:\n",
    "    visualize_ground_truth(data.g)\n",
    "except:\n",
    "    print(\"Visualization skipped (may not work in all environments)\")\n",
    "\n",
    "# Experiment parameters\n",
    "n_ensemble_runs = 20\n",
    "n_particles_svgd = 20\n",
    "n_steps = 2000\n",
    "callback_every = 500\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SETUP\")\n",
    "print(f\"  Deep Ensemble: {n_ensemble_runs} runs × 1 particle each\")\n",
    "print(f\"  SVGD: 1 run × {n_particles_svgd} particles\")\n",
    "print(f\"  Training steps: {n_steps}\")\n",
    "print(f\"  Variables: {data.x.shape[1]}\")\n",
    "print(f\"  Training samples: {data.x.shape[0]}\")\n",
    "print(f\"  Test samples: {data.x_ho.shape[0]}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Storage for results\n",
    "ensemble_results = []\n",
    "ensemble_metrics = {\n",
    "    'eshd_empirical': [],\n",
    "    'auroc_empirical': [],\n",
    "    'negll_empirical': [],\n",
    "    'eshd_mixture': [],\n",
    "    'auroc_mixture': [],\n",
    "    'negll_mixture': [],\n",
    "    'training_time': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEEP ENSEMBLE APPROACH (20 runs × 1 particle)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Deep Ensemble: 20 runs with 1 particle each\n",
    "for run_idx in range(n_ensemble_runs):\n",
    "    print(f\"\\nRun {run_idx + 1}/{n_ensemble_runs}\")\n",
    "    \n",
    "    # Use different seed for each run\n",
    "    key, subk = random.split(key)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create DiBS instance\n",
    "    dibs = JointDiBS(\n",
    "        x=data.x, \n",
    "        interv_mask=None, \n",
    "        graph_model=graph_model, \n",
    "        likelihood_model=likelihood_model\n",
    "    )\n",
    "    \n",
    "    # Sample with 1 particle\n",
    "    gs, thetas = dibs.sample(\n",
    "        key=subk, \n",
    "        n_particles=1, \n",
    "        steps=n_steps, \n",
    "        callback_every=callback_every\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Get distributions\n",
    "    dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "    dibs_mixture = dibs.get_mixture(gs, thetas)\n",
    "    \n",
    "    # Compute metrics\n",
    "    # Empirical\n",
    "    eshd_emp = expected_shd(dist=dibs_empirical, g=data.g)\n",
    "    auroc_emp = threshold_metrics(dist=dibs_empirical, g=data.g)['roc_auc']\n",
    "    negll_emp = neg_ave_log_likelihood(\n",
    "        dist=dibs_empirical, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Mixture\n",
    "    eshd_mix = expected_shd(dist=dibs_mixture, g=data.g)\n",
    "    auroc_mix = threshold_metrics(dist=dibs_mixture, g=data.g)['roc_auc']\n",
    "    negll_mix = neg_ave_log_likelihood(\n",
    "        dist=dibs_mixture, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    run_result = {\n",
    "        'run_idx': run_idx,\n",
    "        'eshd_empirical': eshd_emp,\n",
    "        'auroc_empirical': auroc_emp,\n",
    "        'negll_empirical': negll_emp,\n",
    "        'eshd_mixture': eshd_mix,\n",
    "        'auroc_mixture': auroc_mix,\n",
    "        'negll_mixture': negll_mix,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    ensemble_results.append(run_result)\n",
    "    \n",
    "    # Also store in lists for easy aggregation\n",
    "    ensemble_metrics['eshd_empirical'].append(eshd_emp)\n",
    "    ensemble_metrics['auroc_empirical'].append(auroc_emp)\n",
    "    ensemble_metrics['negll_empirical'].append(negll_emp)\n",
    "    ensemble_metrics['eshd_mixture'].append(eshd_mix)\n",
    "    ensemble_metrics['auroc_mixture'].append(auroc_mix)\n",
    "    ensemble_metrics['negll_mixture'].append(negll_mix)\n",
    "    ensemble_metrics['training_time'].append(training_time)\n",
    "    \n",
    "    print(f\"  Empirical - E-SHD: {eshd_emp:5.2f}, AUROC: {auroc_emp:5.3f}, NegLL: {negll_emp:6.2f}\")\n",
    "    print(f\"  Mixture   - E-SHD: {eshd_mix:5.2f}, AUROC: {auroc_mix:5.3f}, NegLL: {negll_mix:6.2f}\")\n",
    "    print(f\"  Time: {training_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVGD APPROACH (1 run × 20 particles)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SVGD: 1 run with 20 particles\n",
    "key, subk = random.split(key)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dibs_svgd = JointDiBS(\n",
    "    x=data.x, \n",
    "    interv_mask=None, \n",
    "    graph_model=graph_model, \n",
    "    likelihood_model=likelihood_model\n",
    ")\n",
    "\n",
    "gs_svgd, thetas_svgd = dibs_svgd.sample(\n",
    "    key=subk, \n",
    "    n_particles=n_particles_svgd, \n",
    "    steps=n_steps, \n",
    "    callback_every=callback_every\n",
    ")\n",
    "\n",
    "svgd_training_time = time.time() - start_time\n",
    "\n",
    "# Get distributions\n",
    "svgd_empirical = dibs_svgd.get_empirical(gs_svgd, thetas_svgd)\n",
    "svgd_mixture = dibs_svgd.get_mixture(gs_svgd, thetas_svgd)\n",
    "\n",
    "# Compute metrics\n",
    "# Empirical\n",
    "svgd_eshd_emp = expected_shd(dist=svgd_empirical, g=data.g)\n",
    "svgd_auroc_emp = threshold_metrics(dist=svgd_empirical, g=data.g)['roc_auc']\n",
    "svgd_negll_emp = neg_ave_log_likelihood(\n",
    "    dist=svgd_empirical, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "# Mixture\n",
    "svgd_eshd_mix = expected_shd(dist=svgd_mixture, g=data.g)\n",
    "svgd_auroc_mix = threshold_metrics(dist=svgd_mixture, g=data.g)['roc_auc']\n",
    "svgd_negll_mix = neg_ave_log_likelihood(\n",
    "    dist=svgd_mixture, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "print(f\"SVGD Results:\")\n",
    "print(f\"  Empirical - E-SHD: {svgd_eshd_emp:5.2f}, AUROC: {svgd_auroc_emp:5.3f}, NegLL: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Mixture   - E-SHD: {svgd_eshd_mix:5.2f}, AUROC: {svgd_auroc_mix:5.3f}, NegLL: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Time: {svgd_training_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute statistics for ensemble\n",
    "def compute_stats(values):\n",
    "    return {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'median': np.median(values)\n",
    "    }\n",
    "\n",
    "print(\"\\nDEEP ENSEMBLE STATISTICS (20 runs × 1 particle):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for metric_name in ['eshd_empirical', 'auroc_empirical', 'negll_empirical', \n",
    "                   'eshd_mixture', 'auroc_mixture', 'negll_mixture']:\n",
    "    stats = compute_stats(ensemble_metrics[metric_name])\n",
    "    print(f\"{metric_name:15s}: {stats['mean']:6.2f} ± {stats['std']:5.2f} \"\n",
    "          f\"[{stats['min']:5.2f}, {stats['max']:5.2f}] (median: {stats['median']:5.2f})\")\n",
    "\n",
    "training_stats = compute_stats(ensemble_metrics['training_time'])\n",
    "print(f\"{'training_time':15s}: {training_stats['mean']:6.1f} ± {training_stats['std']:5.1f}s \"\n",
    "      f\"(total: {sum(ensemble_metrics['training_time']):.1f}s)\")\n",
    "\n",
    "print(f\"\\nSVGD RESULTS (1 run × 20 particles):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'eshd_empirical':15s}: {svgd_eshd_emp:6.2f}\")\n",
    "print(f\"{'auroc_empirical':15s}: {svgd_auroc_emp:6.2f}\")\n",
    "print(f\"{'negll_empirical':15s}: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"{'eshd_mixture':15s}: {svgd_eshd_mix:6.2f}\")\n",
    "print(f\"{'auroc_mixture':15s}: {svgd_auroc_mix:6.2f}\")\n",
    "print(f\"{'negll_mixture':15s}: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"{'training_time':15s}: {svgd_training_time:6.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare approaches\n",
    "print(\"\\nEMPIRICAL DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "ensemble_mean_eshd_emp = np.mean(ensemble_metrics['eshd_empirical'])\n",
    "ensemble_mean_auroc_emp = np.mean(ensemble_metrics['auroc_empirical'])\n",
    "ensemble_mean_negll_emp = np.mean(ensemble_metrics['negll_empirical'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_emp:5.2f} (± {np.std(ensemble_metrics['eshd_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_emp:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_emp - svgd_eshd_emp:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_emp:5.3f} (± {np.std(ensemble_metrics['auroc_empirical']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_emp:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_emp - svgd_auroc_emp:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_emp:6.2f} (± {np.std(ensemble_metrics['negll_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_emp - svgd_negll_emp:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\nMIXTURE DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "ensemble_mean_eshd_mix = np.mean(ensemble_metrics['eshd_mixture'])\n",
    "ensemble_mean_auroc_mix = np.mean(ensemble_metrics['auroc_mixture'])\n",
    "ensemble_mean_negll_mix = np.mean(ensemble_metrics['negll_mixture'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_mix:5.2f} (± {np.std(ensemble_metrics['eshd_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_mix:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_mix - svgd_eshd_mix:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_mix:5.3f} (± {np.std(ensemble_metrics['auroc_mixture']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_mix:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_mix - svgd_auroc_mix:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_mix:6.2f} (± {np.std(ensemble_metrics['negll_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_mix - svgd_negll_mix:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "better_empirical = []\n",
    "better_mixture = []\n",
    "\n",
    "if ensemble_mean_eshd_emp < svgd_eshd_emp:\n",
    "    better_empirical.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_emp > svgd_auroc_emp:\n",
    "    better_empirical.append(\"AUROC\")\n",
    "if ensemble_mean_negll_emp < svgd_negll_emp:\n",
    "    better_empirical.append(\"NegLL\")\n",
    "\n",
    "if ensemble_mean_eshd_mix < svgd_eshd_mix:\n",
    "    better_mixture.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_mix > svgd_auroc_mix:\n",
    "    better_mixture.append(\"AUROC\")\n",
    "if ensemble_mean_negll_mix < svgd_negll_mix:\n",
    "    better_mixture.append(\"NegLL\")\n",
    "\n",
    "print(f\"Deep Ensemble outperforms SVGD on:\")\n",
    "print(f\"  Empirical distribution: {better_empirical if better_empirical else 'None'}\")\n",
    "print(f\"  Mixture distribution:   {better_mixture if better_mixture else 'None'}\")\n",
    "\n",
    "total_ensemble_time = sum(ensemble_metrics['training_time'])\n",
    "print(f\"\\nComputational efficiency:\")\n",
    "print(f\"  Deep Ensemble total time: {total_ensemble_time:.1f}s\")\n",
    "print(f\"  SVGD time:               {svgd_training_time:.1f}s\")\n",
    "print(f\"  Time ratio (Ensemble/SVGD): {total_ensemble_time/svgd_training_time:.1f}x\")\n",
    "\n",
    "print(f\"\\nThis comparison shows the trade-offs between:\")\n",
    "print(f\"  - Deep Ensemble: Independent optimization, potential for diverse solutions\")\n",
    "print(f\"  - SVGD: Particle interaction, computational efficiency, Bayesian approach\")\n",
    "\n",
    "# Save results for further analysis\n",
    "results_dict = {\n",
    "    'ensemble_results': ensemble_results,\n",
    "    'ensemble_metrics': ensemble_metrics,\n",
    "    'svgd_results': {\n",
    "        'eshd_empirical': svgd_eshd_emp,\n",
    "        'auroc_empirical': svgd_auroc_emp,\n",
    "        'negll_empirical': svgd_negll_emp,\n",
    "        'eshd_mixture': svgd_eshd_mix,\n",
    "        'auroc_mixture': svgd_auroc_mix,\n",
    "        'negll_mixture': svgd_negll_mix,\n",
    "        'training_time': svgd_training_time\n",
    "    },\n",
    "    'ground_truth_edges': np.sum(data.g),\n",
    "    'experiment_params': {\n",
    "        'n_ensemble_runs': n_ensemble_runs,\n",
    "        'n_particles_svgd': n_particles_svgd,\n",
    "        'n_steps': n_steps,\n",
    "        'n_vars': data.x.shape[1],\n",
    "        'n_train_samples': data.x.shape[0],\n",
    "        'n_test_samples': data.x_ho.shape[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nResults stored in 'results_dict' variable for further analysis.\")\n",
    "print(f\"Individual ensemble runs available in 'ensemble_results' list.\")\n",
    "print(f\"Ensemble aggregated metrics available in 'ensemble_metrics' dict.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETED\")\n",
    "print(\"=\"*60) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
