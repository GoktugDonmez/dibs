{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e361100",
   "metadata": {},
   "source": [
    "## Example: Joint inference of $p(G, \\Theta | D)$ for Gaussian Bayes nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d0dfc",
   "metadata": {},
   "source": [
    "Setup for Google Colab. Selecting the **GPU** runtime available in Google colab will make inference significantly faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet dibs-lib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b52aa",
   "metadata": {},
   "source": [
    "DiBS translates the task of inferring the posterior over Bayesian networks into an inference problem over the continuous latent variable $Z$. This is achieved by modeling the directed acyclic graph $G$ of the Bayesian network using the generative model $p(G | Z)$. The prior $p(Z)$ enforces the acyclicity of $G$.\n",
    "Ultimately, this allows us to infer $p(G, \\Theta | D)$ (and $p(G | D)$) using off-the-shelf inference methods such as Stein Variational gradient descent (SVGD) (Liu and Wang, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8370c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.random as random\n",
    "key = random.PRNGKey(123)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5e4a",
   "metadata": {},
   "source": [
    "### Generate synthetic ground truth Bayesian network and BN model for inference\n",
    "\n",
    "`data` contains information about and observations sampled from a synthetic, ground truth causal model with `n_vars` variables. By default, the conditional distributions are linear Gaussian. The random graph model is set by `graph_prior_str`, where `er` denotes Erdos-Renyi and `sf` scale-free graphs. \n",
    "\n",
    "`graph_model` defines prior $p(G)$ and `likelihood_model` defines likelihood $p(x, \\Theta| G ) = p(\\Theta| G )p(x | G, \\Theta )$ of the BN model for which DiBS will infer the posterior.\n",
    "\n",
    "**For posterior inference of nonlinear Gaussian networks parameterized by fully-connected neural networks, use the function `make_nonlinear_gaussian_model`.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98551a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dibs.target import make_linear_gaussian_model, make_nonlinear_gaussian_model\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "key, subk = random.split(key)\n",
    "data, graph_model, likelihood_model = make_linear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "# data, graph_model, likelihood_model = make_nonlinear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "\n",
    "visualize_ground_truth(data.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e46b",
   "metadata": {},
   "source": [
    "### DiBS with SVGD\n",
    "\n",
    "Infer $p(G, \\Theta | D)$ under the prior and conditional distributions defined by the model.\n",
    "The below visualization shows the *matrix of edge probabilities* $G_\\alpha(Z^{(k)})$ implied by each transported latent particle (i.e., sample) $Z^{(k)}$ during the iterations of SVGD with DiBS. Refer to the paper for further details.\n",
    "\n",
    "To explicitly perform posterior inference of $p(G | D)$ using a closed-form marginal likelihood $p(D | G)$, use the separate, analogous class `MarginalDiBS` as demonstrated in the example notebook `dibs_marginal.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ece4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.inference import JointDiBS\n",
    "\n",
    "dibs = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "key, subk = random.split(key)\n",
    "gs, thetas = dibs.sample(key=subk, n_particles=20, steps=2000, callback_every=100, callback=dibs.visualize_callback())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dc8cc",
   "metadata": {},
   "source": [
    "### Evaluate on held-out data\n",
    "\n",
    "Form the empirical (i.e., weighted by counts) and mixture distributions (i.e., weighted by unnormalized posterior probabilities, denoted DiBS+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "dibs_mixture = dibs.get_mixture(gs, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e275383",
   "metadata": {},
   "source": [
    "Compute some evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3dc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "\n",
    "for descr, dist in [('DiBS ', dibs_empirical), ('DiBS+', dibs_mixture)]:\n",
    "    \n",
    "    eshd = expected_shd(dist=dist, g=data.g)        \n",
    "    auroc = threshold_metrics(dist=dist, g=data.g)['roc_auc']\n",
    "    negll = neg_ave_log_likelihood(dist=dist, eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    \n",
    "    print(f'{descr} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}    neg. LL {negll:5.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9f4eb",
   "metadata": {},
   "source": [
    "## Deep Ensemble Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import jax\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "from dibs.target import make_nonlinear_gaussian_model\n",
    "from dibs.inference import JointDiBS\n",
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "main_key = random.PRNGKey(42)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# Generate ground truth nonlinear Gaussian model\n",
    "print(\"Generating ground truth nonlinear Gaussian model...\")\n",
    "key, subk = random.split(main_key)\n",
    "data, graph_model, likelihood_model = make_nonlinear_gaussian_model(\n",
    "    key=subk, \n",
    "    n_vars=20, \n",
    "    graph_prior_str=\"sf\"\n",
    ")\n",
    "\n",
    "print(f\"Ground truth graph has {np.sum(data.g)} edges\")\n",
    "print(\"Visualizing ground truth...\")\n",
    "try:\n",
    "    visualize_ground_truth(data.g)\n",
    "except:\n",
    "    print(\"Visualization skipped (may not work in all environments)\")\n",
    "\n",
    "# Experiment parameters\n",
    "n_ensemble_runs = 20\n",
    "n_particles_svgd = 20\n",
    "n_steps = 2000\n",
    "callback_every = 500\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SETUP\")\n",
    "print(f\"  Deep Ensemble: {n_ensemble_runs} runs × 1 particle each\")\n",
    "print(f\"  SVGD: 1 run × {n_particles_svgd} particles\")\n",
    "print(f\"  Training steps: {n_steps}\")\n",
    "print(f\"  Variables: {data.x.shape[1]}\")\n",
    "print(f\"  Training samples: {data.x.shape[0]}\")\n",
    "print(f\"  Test samples: {data.x_ho.shape[0]}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Storage for results\n",
    "ensemble_results = []\n",
    "ensemble_metrics = {\n",
    "    'eshd_empirical': [],\n",
    "    'auroc_empirical': [],\n",
    "    'negll_empirical': [],\n",
    "    'eshd_mixture': [],\n",
    "    'auroc_mixture': [],\n",
    "    'negll_mixture': [],\n",
    "    'training_time': []\n",
    "}\n",
    "\n",
    "# NEW: Storage for true ensemble (combining all samples)\n",
    "all_ensemble_gs = []\n",
    "all_ensemble_thetas = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEEP ENSEMBLE APPROACH (20 runs × 1 particle)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Deep Ensemble: 20 runs with 1 particle each\n",
    "for run_idx in range(n_ensemble_runs):\n",
    "    print(f\"\\nRun {run_idx + 1}/{n_ensemble_runs}\")\n",
    "    \n",
    "    # Use different seed for each run\n",
    "    key, subk = random.split(key)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create DiBS instance\n",
    "    dibs = JointDiBS(\n",
    "        x=data.x, \n",
    "        interv_mask=None, \n",
    "        graph_model=graph_model, \n",
    "        likelihood_model=likelihood_model\n",
    "    )\n",
    "    \n",
    "    # Sample with 1 particle\n",
    "    gs, thetas = dibs.sample(\n",
    "        key=subk, \n",
    "        n_particles=1, \n",
    "        steps=n_steps, \n",
    "        callback_every=callback_every\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Get distributions\n",
    "    dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "    dibs_mixture = dibs.get_mixture(gs, thetas)\n",
    "    \n",
    "    # Compute metrics\n",
    "    # Empirical\n",
    "    eshd_emp = expected_shd(dist=dibs_empirical, g=data.g)\n",
    "    auroc_emp = threshold_metrics(dist=dibs_empirical, g=data.g)['roc_auc']\n",
    "    negll_emp = neg_ave_log_likelihood(\n",
    "        dist=dibs_empirical, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Mixture\n",
    "    eshd_mix = expected_shd(dist=dibs_mixture, g=data.g)\n",
    "    auroc_mix = threshold_metrics(dist=dibs_mixture, g=data.g)['roc_auc']\n",
    "    negll_mix = neg_ave_log_likelihood(\n",
    "        dist=dibs_mixture, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    run_result = {\n",
    "        'run_idx': run_idx,\n",
    "        'eshd_empirical': eshd_emp,\n",
    "        'auroc_empirical': auroc_emp,\n",
    "        'negll_empirical': negll_emp,\n",
    "        'eshd_mixture': eshd_mix,\n",
    "        'auroc_mixture': auroc_mix,\n",
    "        'negll_mixture': negll_mix,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    ensemble_results.append(run_result)\n",
    "    \n",
    "    # Also store in lists for easy aggregation\n",
    "    ensemble_metrics['eshd_empirical'].append(eshd_emp)\n",
    "    ensemble_metrics['auroc_empirical'].append(auroc_emp)\n",
    "    ensemble_metrics['negll_empirical'].append(negll_emp)\n",
    "    ensemble_metrics['eshd_mixture'].append(eshd_mix)\n",
    "    ensemble_metrics['auroc_mixture'].append(auroc_mix)\n",
    "    ensemble_metrics['negll_mixture'].append(negll_mix)\n",
    "    ensemble_metrics['training_time'].append(training_time)\n",
    "    \n",
    "    # NEW: Store samples for true ensemble\n",
    "    all_ensemble_gs.append(gs)\n",
    "    all_ensemble_thetas.append(thetas)\n",
    "    \n",
    "    print(f\"  Empirical - E-SHD: {eshd_emp:5.2f}, AUROC: {auroc_emp:5.3f}, NegLL: {negll_emp:6.2f}\")\n",
    "    print(f\"  Mixture   - E-SHD: {eshd_mix:5.2f}, AUROC: {auroc_mix:5.3f}, NegLL: {negll_mix:6.2f}\")\n",
    "    print(f\"  Time: {training_time:.1f}s\")\n",
    "\n",
    "# NEW: Compute TRUE ENSEMBLE by combining ALL samples from all runs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRUE DEEP ENSEMBLE (combining all 20 samples)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine all graphs and parameters from all runs into single arrays\n",
    "combined_gs = np.concatenate(all_ensemble_gs, axis=0)  # [20, d, d] \n",
    "combined_thetas = jax.tree_map(lambda *arrays: np.concatenate(arrays, axis=0), *all_ensemble_thetas)\n",
    "\n",
    "print(f\"Combined ensemble contains {combined_gs.shape[0]} total samples\")\n",
    "\n",
    "# Create a single DiBS instance to compute distributions (any will work since we're just using the method)\n",
    "dibs_for_ensemble = JointDiBS(\n",
    "    x=data.x, \n",
    "    interv_mask=None, \n",
    "    graph_model=graph_model, \n",
    "    likelihood_model=likelihood_model\n",
    ")\n",
    "\n",
    "# Get true ensemble distributions\n",
    "true_ensemble_empirical = dibs_for_ensemble.get_empirical(combined_gs, combined_thetas)\n",
    "true_ensemble_mixture = dibs_for_ensemble.get_mixture(combined_gs, combined_thetas)\n",
    "\n",
    "# Compute metrics on true ensemble\n",
    "true_eshd_emp = expected_shd(dist=true_ensemble_empirical, g=data.g)\n",
    "true_auroc_emp = threshold_metrics(dist=true_ensemble_empirical, g=data.g)['roc_auc']\n",
    "true_negll_emp = neg_ave_log_likelihood(\n",
    "    dist=true_ensemble_empirical, \n",
    "    eltwise_log_likelihood=dibs_for_ensemble.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "true_eshd_mix = expected_shd(dist=true_ensemble_mixture, g=data.g)\n",
    "true_auroc_mix = threshold_metrics(dist=true_ensemble_mixture, g=data.g)['roc_auc']\n",
    "true_negll_mix = neg_ave_log_likelihood(\n",
    "    dist=true_ensemble_mixture, \n",
    "    eltwise_log_likelihood=dibs_for_ensemble.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "print(f\"TRUE ENSEMBLE Results:\")\n",
    "print(f\"  Empirical - E-SHD: {true_eshd_emp:5.2f}, AUROC: {true_auroc_emp:5.3f}, NegLL: {true_negll_emp:6.2f}\")\n",
    "print(f\"  Mixture   - E-SHD: {true_eshd_mix:5.2f}, AUROC: {true_auroc_mix:5.3f}, NegLL: {true_negll_mix:6.2f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVGD APPROACH (1 run × 20 particles)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SVGD: 1 run with 20 particles\n",
    "key, subk = random.split(key)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dibs_svgd = JointDiBS(\n",
    "    x=data.x, \n",
    "    interv_mask=None, \n",
    "    graph_model=graph_model, \n",
    "    likelihood_model=likelihood_model\n",
    ")\n",
    "\n",
    "gs_svgd, thetas_svgd = dibs_svgd.sample(\n",
    "    key=subk, \n",
    "    n_particles=n_particles_svgd, \n",
    "    steps=n_steps, \n",
    "    callback_every=callback_every\n",
    ")\n",
    "\n",
    "svgd_training_time = time.time() - start_time\n",
    "\n",
    "# Get distributions\n",
    "svgd_empirical = dibs_svgd.get_empirical(gs_svgd, thetas_svgd)\n",
    "svgd_mixture = dibs_svgd.get_mixture(gs_svgd, thetas_svgd)\n",
    "\n",
    "# Compute metrics\n",
    "# Empirical\n",
    "svgd_eshd_emp = expected_shd(dist=svgd_empirical, g=data.g)\n",
    "svgd_auroc_emp = threshold_metrics(dist=svgd_empirical, g=data.g)['roc_auc']\n",
    "svgd_negll_emp = neg_ave_log_likelihood(\n",
    "    dist=svgd_empirical, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "# Mixture\n",
    "svgd_eshd_mix = expected_shd(dist=svgd_mixture, g=data.g)\n",
    "svgd_auroc_mix = threshold_metrics(dist=svgd_mixture, g=data.g)['roc_auc']\n",
    "svgd_negll_mix = neg_ave_log_likelihood(\n",
    "    dist=svgd_mixture, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "print(f\"SVGD Results:\")\n",
    "print(f\"  Empirical - E-SHD: {svgd_eshd_emp:5.2f}, AUROC: {svgd_auroc_emp:5.3f}, NegLL: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Mixture   - E-SHD: {svgd_eshd_mix:5.2f}, AUROC: {svgd_auroc_mix:5.3f}, NegLL: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Time: {svgd_training_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute statistics for ensemble\n",
    "def compute_stats(values):\n",
    "    return {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'median': np.median(values)\n",
    "    }\n",
    "\n",
    "print(\"\\nDEEP ENSEMBLE STATISTICS - AVERAGE OF INDIVIDUALS (20 runs × 1 particle):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for metric_name in ['eshd_empirical', 'auroc_empirical', 'negll_empirical', \n",
    "                   'eshd_mixture', 'auroc_mixture', 'negll_mixture']:\n",
    "    stats = compute_stats(ensemble_metrics[metric_name])\n",
    "    print(f\"{metric_name:15s}: {stats['mean']:6.2f} ± {stats['std']:5.2f} \"\n",
    "          f\"[{stats['min']:5.2f}, {stats['max']:5.2f}] (median: {stats['median']:5.2f})\")\n",
    "\n",
    "training_stats = compute_stats(ensemble_metrics['training_time'])\n",
    "print(f\"{'training_time':15s}: {training_stats['mean']:6.1f} ± {training_stats['std']:5.1f}s \"\n",
    "      f\"(total: {sum(ensemble_metrics['training_time']):.1f}s)\")\n",
    "\n",
    "print(f\"\\nTRUE DEEP ENSEMBLE STATISTICS (combined 20 samples):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'eshd_empirical':15s}: {true_eshd_emp:6.2f}\")\n",
    "print(f\"{'auroc_empirical':15s}: {true_auroc_emp:6.2f}\")\n",
    "print(f\"{'negll_empirical':15s}: {true_negll_emp:6.2f}\")\n",
    "print(f\"{'eshd_mixture':15s}: {true_eshd_mix:6.2f}\")\n",
    "print(f\"{'auroc_mixture':15s}: {true_auroc_mix:6.2f}\")\n",
    "print(f\"{'negll_mixture':15s}: {true_negll_mix:6.2f}\")\n",
    "\n",
    "print(f\"\\nSVGD RESULTS (1 run × 20 particles):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'eshd_empirical':15s}: {svgd_eshd_emp:6.2f}\")\n",
    "print(f\"{'auroc_empirical':15s}: {svgd_auroc_emp:6.2f}\")\n",
    "print(f\"{'negll_empirical':15s}: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"{'eshd_mixture':15s}: {svgd_eshd_mix:6.2f}\")\n",
    "print(f\"{'auroc_mixture':15s}: {svgd_auroc_mix:6.2f}\")\n",
    "print(f\"{'negll_mixture':15s}: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"{'training_time':15s}: {svgd_training_time:6.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare approaches\n",
    "print(\"\\nEMPIRICAL DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"(A) AVERAGE-OF-INDIVIDUALS vs SVGD:\")\n",
    "ensemble_mean_eshd_emp = np.mean(ensemble_metrics['eshd_empirical'])\n",
    "ensemble_mean_auroc_emp = np.mean(ensemble_metrics['auroc_empirical'])\n",
    "ensemble_mean_negll_emp = np.mean(ensemble_metrics['negll_empirical'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_emp:5.2f} (± {np.std(ensemble_metrics['eshd_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_emp:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_emp - svgd_eshd_emp:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_emp:5.3f} (± {np.std(ensemble_metrics['auroc_empirical']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_emp:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_emp - svgd_auroc_emp:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_emp:6.2f} (± {np.std(ensemble_metrics['negll_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_emp - svgd_negll_emp:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\n(B) TRUE-ENSEMBLE vs SVGD:\")\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  True Ensemble: {true_eshd_emp:5.2f}\")\n",
    "print(f\"  SVGD:          {svgd_eshd_emp:5.2f}\")\n",
    "print(f\"  Difference:    {true_eshd_emp - svgd_eshd_emp:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  True Ensemble: {true_auroc_emp:5.3f}\")\n",
    "print(f\"  SVGD:          {svgd_auroc_emp:5.3f}\")\n",
    "print(f\"  Difference:    {true_auroc_emp - svgd_auroc_emp:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  True Ensemble: {true_negll_emp:6.2f}\")\n",
    "print(f\"  SVGD:          {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Difference:    {true_negll_emp - svgd_negll_emp:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\nMIXTURE DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"(A) AVERAGE-OF-INDIVIDUALS vs SVGD:\")\n",
    "ensemble_mean_eshd_mix = np.mean(ensemble_metrics['eshd_mixture'])\n",
    "ensemble_mean_auroc_mix = np.mean(ensemble_metrics['auroc_mixture'])\n",
    "ensemble_mean_negll_mix = np.mean(ensemble_metrics['negll_mixture'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_mix:5.2f} (± {np.std(ensemble_metrics['eshd_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_mix:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_mix - svgd_eshd_mix:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_mix:5.3f} (± {np.std(ensemble_metrics['auroc_mixture']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_mix:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_mix - svgd_auroc_mix:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_mix:6.2f} (± {np.std(ensemble_metrics['negll_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_mix - svgd_negll_mix:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\n(B) TRUE-ENSEMBLE vs SVGD:\")\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  True Ensemble: {true_eshd_mix:5.2f}\")\n",
    "print(f\"  SVGD:          {svgd_eshd_mix:5.2f}\")\n",
    "print(f\"  Difference:    {true_eshd_mix - svgd_eshd_mix:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  True Ensemble: {true_auroc_mix:5.3f}\")\n",
    "print(f\"  SVGD:          {svgd_auroc_mix:5.3f}\")\n",
    "print(f\"  Difference:    {true_auroc_mix - svgd_auroc_mix:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  True Ensemble: {true_negll_mix:6.2f}\")\n",
    "print(f\"  SVGD:          {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Difference:    {true_negll_mix - svgd_negll_mix:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average-of-individuals comparison\n",
    "better_empirical_avg = []\n",
    "better_mixture_avg = []\n",
    "\n",
    "if ensemble_mean_eshd_emp < svgd_eshd_emp:\n",
    "    better_empirical_avg.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_emp > svgd_auroc_emp:\n",
    "    better_empirical_avg.append(\"AUROC\")\n",
    "if ensemble_mean_negll_emp < svgd_negll_emp:\n",
    "    better_empirical_avg.append(\"NegLL\")\n",
    "\n",
    "if ensemble_mean_eshd_mix < svgd_eshd_mix:\n",
    "    better_mixture_avg.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_mix > svgd_auroc_mix:\n",
    "    better_mixture_avg.append(\"AUROC\")\n",
    "if ensemble_mean_negll_mix < svgd_negll_mix:\n",
    "    better_mixture_avg.append(\"NegLL\")\n",
    "\n",
    "# True ensemble comparison\n",
    "better_empirical_true = []\n",
    "better_mixture_true = []\n",
    "\n",
    "if true_eshd_emp < svgd_eshd_emp:\n",
    "    better_empirical_true.append(\"E-SHD\")\n",
    "if true_auroc_emp > svgd_auroc_emp:\n",
    "    better_empirical_true.append(\"AUROC\")\n",
    "if true_negll_emp < svgd_negll_emp:\n",
    "    better_empirical_true.append(\"NegLL\")\n",
    "\n",
    "if true_eshd_mix < svgd_eshd_mix:\n",
    "    better_mixture_true.append(\"E-SHD\")\n",
    "if true_auroc_mix > svgd_auroc_mix:\n",
    "    better_mixture_true.append(\"AUROC\")\n",
    "if true_negll_mix < svgd_negll_mix:\n",
    "    better_mixture_true.append(\"NegLL\")\n",
    "\n",
    "print(f\"AVERAGE-OF-INDIVIDUALS Deep Ensemble outperforms SVGD on:\")\n",
    "print(f\"  Empirical distribution: {better_empirical_avg if better_empirical_avg else 'None'}\")\n",
    "print(f\"  Mixture distribution:   {better_mixture_avg if better_mixture_avg else 'None'}\")\n",
    "\n",
    "print(f\"\\nTRUE Deep Ensemble outperforms SVGD on:\")\n",
    "print(f\"  Empirical distribution: {better_empirical_true if better_empirical_true else 'None'}\")\n",
    "print(f\"  Mixture distribution:   {better_mixture_true if better_mixture_true else 'None'}\")\n",
    "\n",
    "total_ensemble_time = sum(ensemble_metrics['training_time'])\n",
    "print(f\"\\nComputational efficiency:\")\n",
    "print(f\"  Deep Ensemble total time: {total_ensemble_time:.1f}s\")\n",
    "print(f\"  SVGD time:               {svgd_training_time:.1f}s\")\n",
    "print(f\"  Time ratio (Ensemble/SVGD): {total_ensemble_time/svgd_training_time:.1f}x\")\n",
    "\n",
    "print(f\"\\nIMPORTANT: This comparison demonstrates two different ways to use deep ensembles:\")\n",
    "print(f\"  - AVERAGE-OF-INDIVIDUALS: Average the performance metrics across runs\")\n",
    "print(f\"  - TRUE ENSEMBLE: Combine all samples into one distribution, then evaluate\")\n",
    "print(f\"  - SVGD: Particle interaction for Bayesian inference\")\n",
    "print(f\"\\nThe TRUE ENSEMBLE approach is the proper way to evaluate ensemble methods!\")\n",
    "print(f\"Averaging individual performances != Performance of the ensemble!\")\n",
    "\n",
    "# Save results for further analysis\n",
    "results_dict = {\n",
    "    'ensemble_results': ensemble_results,\n",
    "    'ensemble_metrics': ensemble_metrics,\n",
    "    'true_ensemble_results': {\n",
    "        'eshd_empirical': true_eshd_emp,\n",
    "        'auroc_empirical': true_auroc_emp,\n",
    "        'negll_empirical': true_negll_emp,\n",
    "        'eshd_mixture': true_eshd_mix,\n",
    "        'auroc_mixture': true_auroc_mix,\n",
    "        'negll_mixture': true_negll_mix,\n",
    "        'combined_samples': combined_gs.shape[0]\n",
    "    },\n",
    "    'svgd_results': {\n",
    "        'eshd_empirical': svgd_eshd_emp,\n",
    "        'auroc_empirical': svgd_auroc_emp,\n",
    "        'negll_empirical': svgd_negll_emp,\n",
    "        'eshd_mixture': svgd_eshd_mix,\n",
    "        'auroc_mixture': svgd_auroc_mix,\n",
    "        'negll_mixture': svgd_negll_mix,\n",
    "        'training_time': svgd_training_time\n",
    "    },\n",
    "    'ground_truth_edges': np.sum(data.g),\n",
    "    'experiment_params': {\n",
    "        'n_ensemble_runs': n_ensemble_runs,\n",
    "        'n_particles_svgd': n_particles_svgd,\n",
    "        'n_steps': n_steps,\n",
    "        'n_vars': data.x.shape[1],\n",
    "        'n_train_samples': data.x.shape[0],\n",
    "        'n_test_samples': data.x_ho.shape[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nResults stored in 'results_dict' variable for further analysis.\")\n",
    "print(f\"Individual ensemble runs available in 'ensemble_results' list.\")\n",
    "print(f\"Ensemble aggregated metrics available in 'ensemble_metrics' dict.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETED\")\n",
    "print(\"=\"*60) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Ensemble Experiments for Causal Discovery with Interventional Data (Principled Approach)\n",
    "\n",
    "This script compares SVGD and Deep Ensembles on their ability to recover\n",
    "a causal graph when trained and evaluated on a mix of observational and\n",
    "single-target interventional data.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.random as random\n",
    "import jax.tree_util\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import jax.numpy as jnp\n",
    "import igraph as ig\n",
    "from jax import vmap\n",
    "\n",
    "from dibs.target import make_synthetic_bayes_net, make_graph_model\n",
    "from dibs.models import DenseNonlinearGaussian\n",
    "from dibs.inference import JointDiBS\n",
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "\n",
    "def create_interventional_data(key, n_vars, n_observations, n_ho_observations, n_intervention_sets, perc_intervened):\n",
    "    \"\"\"\n",
    "    Generates synthetic data, combining observational and interventional for training,\n",
    "    but keeping observational and interventional held-out sets separate.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"1. GENERATING GROUND TRUTH DATA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    key, subk = random.split(key)\n",
    "\n",
    "    # Define graph model and generative/likelihood models\n",
    "    graph_model = make_graph_model(n_vars=n_vars, graph_prior_str=\"sf\")\n",
    "    generative_model = DenseNonlinearGaussian(\n",
    "        n_vars=n_vars, hidden_layers=(5,), obs_noise=0.1, sig_param=1.0)\n",
    "    likelihood_model = DenseNonlinearGaussian(\n",
    "        n_vars=n_vars, hidden_layers=(5,), obs_noise=0.1, sig_param=1.0)\n",
    "\n",
    "    # Generate all intervention sets\n",
    "    data = make_synthetic_bayes_net(\n",
    "        key=subk, n_vars=n_vars, graph_model=graph_model, generative_model=generative_model,\n",
    "        n_observations=n_observations, n_ho_observations=n_ho_observations,\n",
    "        n_intervention_sets=n_intervention_sets, perc_intervened=perc_intervened)\n",
    "\n",
    "    # --- Create Combined Training Dataset ---\n",
    "    # Start with observational data\n",
    "    all_train_data = [data.x]\n",
    "    all_train_masks = [jnp.zeros_like(data.x, dtype=bool)]\n",
    "\n",
    "    # Add first (n_intervention_sets-1) interventional datasets for training\n",
    "    for i in range(n_intervention_sets - 1):\n",
    "        interv_dict, interv_x_train = data.x_interv[i]\n",
    "        all_train_data.append(interv_x_train)\n",
    "        \n",
    "        # Create intervention mask\n",
    "        mask_train_interv = jnp.zeros_like(interv_x_train, dtype=bool)\n",
    "        intervened_nodes = list(interv_dict.keys())\n",
    "        mask_train_interv = mask_train_interv.at[:, intervened_nodes].set(True)\n",
    "        all_train_masks.append(mask_train_interv)\n",
    "\n",
    "    # Finalize training set\n",
    "    x_train = jnp.concatenate(all_train_data, axis=0)\n",
    "    mask_train = jnp.concatenate(all_train_masks, axis=0)\n",
    "\n",
    "    # --- Prepare Held-Out Sets ---\n",
    "    # Observational held-out (already available)\n",
    "    x_ho_obs = data.x_ho\n",
    "\n",
    "    # Interventional held-out (use last intervention set)\n",
    "    interv_dict_ho, x_ho_intrv = data.x_interv[-1]\n",
    "    mask_ho_intrv = jnp.zeros_like(x_ho_intrv, dtype=bool)\n",
    "    intervened_nodes_ho = list(interv_dict_ho.keys())\n",
    "    mask_ho_intrv = mask_ho_intrv.at[:, intervened_nodes_ho].set(True)\n",
    "\n",
    "    print(f\"Ground truth graph generated.\")\n",
    "    print(f\"\\nTotal training samples: {x_train.shape[0]} (observational + {n_intervention_sets-1} interventional sets)\")\n",
    "    print(f\"Held-out observational samples: {x_ho_obs.shape[0]}\")\n",
    "    print(f\"Held-out interventional samples: {x_ho_intrv.shape[0]}\")\n",
    "\n",
    "    return (x_train, mask_train, x_ho_obs, x_ho_intrv, mask_ho_intrv,\n",
    "            data.g, graph_model, likelihood_model, data)\n",
    "\n",
    "def compute_metrics(dist, name, dibs_instance, x_ho, mask_ho, g_true):\n",
    "    \"\"\"Computes and prints metrics for a given particle distribution.\"\"\"\n",
    "    eshd = expected_shd(dist=dist, g=g_true)\n",
    "    auroc = threshold_metrics(dist=dist, g=g_true)['roc_auc']\n",
    "    \n",
    "    # Compute negative average log-likelihood\n",
    "    negll = neg_ave_log_likelihood(\n",
    "        dist=dist, \n",
    "        eltwise_log_likelihood=lambda g, theta: dibs_instance.likelihood_model.log_prob(g=g, theta=theta, x=x_ho, interv_mask=mask_ho),\n",
    "        x=None  # Not used since we provide lambda with x and mask baked in\n",
    "    )\n",
    "\n",
    "    print(f'{name:25s} | E-SHD: {eshd:5.2f}  AUROC: {auroc:5.3f}  NegLL: {negll:7.2f}')\n",
    "    return {'eshd': eshd, 'auroc': auroc, 'negll': negll}\n",
    "\n",
    "# --- SCRIPT START ---\n",
    "key = random.PRNGKey(42)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# 1. Generate Data\n",
    "(x_train, mask_train, x_ho_obs, x_ho_intrv, mask_ho_intrv,\n",
    " g_true, graph_model, likelihood_model, data) = create_interventional_data(\n",
    "    key=key, n_vars=20, n_observations=100, n_ho_observations=100,\n",
    "    n_intervention_sets=10, perc_intervened=0.1)\n",
    "\n",
    "# 2. Experiment Parameters\n",
    "N_PARTICLES = 20\n",
    "N_ENSEMBLE_RUNS = 20\n",
    "N_STEPS = 2000\n",
    "\n",
    "# 3. SVGD Baseline (1 run × 20 particles)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"3. SVGD BASELINE (1 run x {N_PARTICLES} particles)\")\n",
    "print(\"=\"*70)\n",
    "key, subk = random.split(key)\n",
    "dibs_svgd = JointDiBS(x=x_train, interv_mask=mask_train, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "\n",
    "start_time = time.time()\n",
    "gs_svgd, thetas_svgd = dibs_svgd.sample(key=subk, n_particles=N_PARTICLES, steps=N_STEPS)\n",
    "svgd_time = time.time() - start_time\n",
    "\n",
    "svgd_mixture = dibs_svgd.get_mixture(gs_svgd, thetas_svgd)\n",
    "print(f\"Finished in {svgd_time:.2f}s\")\n",
    "\n",
    "# 4. Deep Ensemble (20 runs × 1 particle)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"4. DEEP ENSEMBLE ({N_ENSEMBLE_RUNS} runs x 1 particle)\")\n",
    "print(\"=\"*70)\n",
    "ensemble_gs = []\n",
    "ensemble_thetas = []\n",
    "\n",
    "ensemble_start = time.time()\n",
    "for i in range(N_ENSEMBLE_RUNS):\n",
    "    print(f\"Run {i+1}/{N_ENSEMBLE_RUNS}\", end=\" \")\n",
    "    key, subk = random.split(key)\n",
    "    dibs_single = JointDiBS(x=x_train, interv_mask=mask_train, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "    gs, thetas = dibs_single.sample(key=subk, n_particles=1, steps=N_STEPS)\n",
    "    ensemble_gs.append(gs)\n",
    "    ensemble_thetas.append(thetas)\n",
    "    print(\"✓\")\n",
    "ensemble_time = time.time() - ensemble_start\n",
    "\n",
    "combined_gs = np.concatenate(ensemble_gs, axis=0)\n",
    "combined_thetas = jax.tree_util.tree_map(lambda *arrays: np.concatenate(arrays, axis=0), *ensemble_thetas)\n",
    "\n",
    "dibs_ensemble = JointDiBS(x=x_train, interv_mask=mask_train, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "true_ensemble_mixture = dibs_ensemble.get_mixture(combined_gs, combined_thetas)\n",
    "print(f\"Finished in {ensemble_time:.2f}s\")\n",
    "\n",
    "# --- 5. Evaluation ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5. EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute structural metrics once (they are the same for obs and intrv)\n",
    "svgd_eshd = expected_shd(dist=svgd_mixture, g=g_true)\n",
    "svgd_auroc = threshold_metrics(dist=svgd_mixture, g=g_true)['roc_auc']\n",
    "\n",
    "ens_eshd = expected_shd(dist=true_ensemble_mixture, g=g_true)\n",
    "ens_auroc = threshold_metrics(dist=true_ensemble_mixture, g=g_true)['roc_auc']\n",
    "\n",
    "# --- 5a. On OBSERVATIONAL Held-Out Data ---\n",
    "print(\"--- 5a. On OBSERVATIONAL Held-Out Data ---\")\n",
    "svgd_negll_obs = neg_ave_log_likelihood(\n",
    "    dist=svgd_mixture,\n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ,\n",
    "    x=x_ho_obs\n",
    ")\n",
    "ens_negll_obs = neg_ave_log_likelihood(\n",
    "    dist=true_ensemble_mixture,\n",
    "    eltwise_log_likelihood=dibs_ensemble.eltwise_log_likelihood_observ,\n",
    "    x=x_ho_obs\n",
    ")\n",
    "print(f'SVGD Mixture (Obs)      | E-SHD: {svgd_eshd:5.2f}  AUROC: {svgd_auroc:5.3f}  NegLL: {svgd_negll_obs:7.2f}')\n",
    "print(f'Ensemble Mixture (Obs)  | E-SHD: {ens_eshd:5.2f}  AUROC: {ens_auroc:5.3f}  NegLL: {ens_negll_obs:7.2f}')\n",
    "\n",
    "# --- 5b. On INTERVENTIONAL Held-Out Data ---\n",
    "print(\"\\n--- 5b. On INTERVENTIONAL Held-Out Data ---\")\n",
    "svgd_negll_intrv = neg_ave_log_likelihood(\n",
    "    dist=svgd_mixture,\n",
    "    eltwise_log_likelihood=lambda g, theta, x: dibs_svgd.eltwise_log_likelihood_interv(g, theta, x, mask_ho_intrv),\n",
    "    x=x_ho_intrv\n",
    ")\n",
    "ens_negll_intrv = neg_ave_log_likelihood(\n",
    "    dist=true_ensemble_mixture,\n",
    "    eltwise_log_likelihood=lambda g, theta, x: dibs_ensemble.eltwise_log_likelihood_interv(g, theta, x, mask_ho_intrv),\n",
    "    x=x_ho_intrv\n",
    ")\n",
    "print(f'SVGD Mixture (Intrv)    | E-SHD: {svgd_eshd:5.2f}  AUROC: {svgd_auroc:5.3f}  NegLL: {svgd_negll_intrv:7.2f}')\n",
    "print(f'Ensemble Mixture (Intrv)| E-SHD: {ens_eshd:5.2f}  AUROC: {ens_auroc:5.3f}  NegLL: {ens_negll_intrv:7.2f}')\n",
    "\n",
    "# 6. Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"6. SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Computation time:\")\n",
    "print(f\"  SVGD ({N_PARTICLES} particles):      {svgd_time:6.1f}s\")\n",
    "print(f\"  Deep Ensemble ({N_ENSEMBLE_RUNS} × 1):   {ensemble_time:6.1f}s\")\n",
    "print(\"\\n{:<25} | {:>10} | {:>10}\".format(\"Metric\", \"SVGD\", \"Ensemble\"))\n",
    "print(\"-\"*51)\n",
    "print(\"{:<25} | {:10.2f} | {:10.2f}\".format(\"E-SHD\", svgd_eshd, ens_eshd))\n",
    "print(\"{:<25} | {:10.3f} | {:10.3f}\".format(\"AUROC\", svgd_auroc, ens_auroc))\n",
    "print(\"{:<25} | {:10.2f} | {:10.2f}\".format(\"NLL (Observational)\", svgd_negll_obs, ens_negll_obs))\n",
    "print(\"{:<25} | {:10.2f} | {:10.2f}\".format(\"NLL (Interventional)\", svgd_negll_intrv, ens_negll_intrv))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 7. Save Results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"7. SAVING RESULTS TO experiment_results.csv\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_path = \"experiment_results.csv\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    f.write(\"Method,ESHD,AUROC,NLL_Observational,NLL_Interventional,Time_sec\\n\")\n",
    "    f.write(f\"SVGD,{svgd_eshd:.4f},{svgd_auroc:.4f},{svgd_negll_obs:.2f},{svgd_negll_intrv:.2f},{svgd_time:.2f}\\n\")\n",
    "    f.write(f\"Ensemble,{ens_eshd:.4f},{ens_auroc:.4f},{ens_negll_obs:.2f},{ens_negll_intrv:.2f},{ensemble_time:.2f}\\n\")\n",
    "print(f\"Results successfully saved to {results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
