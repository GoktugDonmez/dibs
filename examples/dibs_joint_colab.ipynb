{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e361100",
   "metadata": {},
   "source": [
    "## Example: Joint inference of $p(G, \\Theta | D)$ for Gaussian Bayes nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d0dfc",
   "metadata": {},
   "source": [
    "Setup for Google Colab. Selecting the **GPU** runtime available in Google colab will make inference significantly faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet dibs-lib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b52aa",
   "metadata": {},
   "source": [
    "DiBS translates the task of inferring the posterior over Bayesian networks into an inference problem over the continuous latent variable $Z$. This is achieved by modeling the directed acyclic graph $G$ of the Bayesian network using the generative model $p(G | Z)$. The prior $p(Z)$ enforces the acyclicity of $G$.\n",
    "Ultimately, this allows us to infer $p(G, \\Theta | D)$ (and $p(G | D)$) using off-the-shelf inference methods such as Stein Variational gradient descent (SVGD) (Liu and Wang, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8370c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.random as random\n",
    "key = random.PRNGKey(123)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5e4a",
   "metadata": {},
   "source": [
    "### Generate synthetic ground truth Bayesian network and BN model for inference\n",
    "\n",
    "`data` contains information about and observations sampled from a synthetic, ground truth causal model with `n_vars` variables. By default, the conditional distributions are linear Gaussian. The random graph model is set by `graph_prior_str`, where `er` denotes Erdos-Renyi and `sf` scale-free graphs. \n",
    "\n",
    "`graph_model` defines prior $p(G)$ and `likelihood_model` defines likelihood $p(x, \\Theta| G ) = p(\\Theta| G )p(x | G, \\Theta )$ of the BN model for which DiBS will infer the posterior.\n",
    "\n",
    "**For posterior inference of nonlinear Gaussian networks parameterized by fully-connected neural networks, use the function `make_nonlinear_gaussian_model`.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98551a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dibs.target import make_linear_gaussian_model, make_nonlinear_gaussian_model\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "key, subk = random.split(key)\n",
    "data, graph_model, likelihood_model = make_linear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "# data, graph_model, likelihood_model = make_nonlinear_gaussian_model(key=subk, n_vars=20, graph_prior_str=\"sf\")\n",
    "\n",
    "visualize_ground_truth(data.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e46b",
   "metadata": {},
   "source": [
    "### DiBS with SVGD\n",
    "\n",
    "Infer $p(G, \\Theta | D)$ under the prior and conditional distributions defined by the model.\n",
    "The below visualization shows the *matrix of edge probabilities* $G_\\alpha(Z^{(k)})$ implied by each transported latent particle (i.e., sample) $Z^{(k)}$ during the iterations of SVGD with DiBS. Refer to the paper for further details.\n",
    "\n",
    "To explicitly perform posterior inference of $p(G | D)$ using a closed-form marginal likelihood $p(D | G)$, use the separate, analogous class `MarginalDiBS` as demonstrated in the example notebook `dibs_marginal.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ece4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.inference import JointDiBS\n",
    "\n",
    "dibs = JointDiBS(x=data.x, interv_mask=None, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "key, subk = random.split(key)\n",
    "gs, thetas = dibs.sample(key=subk, n_particles=20, steps=2000, callback_every=100, callback=dibs.visualize_callback())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dc8cc",
   "metadata": {},
   "source": [
    "### Evaluate on held-out data\n",
    "\n",
    "Form the empirical (i.e., weighted by counts) and mixture distributions (i.e., weighted by unnormalized posterior probabilities, denoted DiBS+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "dibs_mixture = dibs.get_mixture(gs, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e275383",
   "metadata": {},
   "source": [
    "Compute some evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3dc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "\n",
    "for descr, dist in [('DiBS ', dibs_empirical), ('DiBS+', dibs_mixture)]:\n",
    "    \n",
    "    eshd = expected_shd(dist=dist, g=data.g)        \n",
    "    auroc = threshold_metrics(dist=dist, g=data.g)['roc_auc']\n",
    "    negll = neg_ave_log_likelihood(dist=dist, eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, x=data.x_ho)\n",
    "    \n",
    "    print(f'{descr} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}    neg. LL {negll:5.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9f4eb",
   "metadata": {},
   "source": [
    "## Deep Ensemble Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import jax\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "from dibs.target import make_nonlinear_gaussian_model\n",
    "from dibs.inference import JointDiBS\n",
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "main_key = random.PRNGKey(42)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# Generate ground truth nonlinear Gaussian model\n",
    "print(\"Generating ground truth nonlinear Gaussian model...\")\n",
    "key, subk = random.split(main_key)\n",
    "data, graph_model, likelihood_model = make_nonlinear_gaussian_model(\n",
    "    key=subk, \n",
    "    n_vars=20, \n",
    "    graph_prior_str=\"sf\"\n",
    ")\n",
    "\n",
    "print(f\"Ground truth graph has {np.sum(data.g)} edges\")\n",
    "print(\"Visualizing ground truth...\")\n",
    "try:\n",
    "    visualize_ground_truth(data.g)\n",
    "except:\n",
    "    print(\"Visualization skipped (may not work in all environments)\")\n",
    "\n",
    "# Experiment parameters\n",
    "n_ensemble_runs = 20\n",
    "n_particles_svgd = 20\n",
    "n_steps = 2000\n",
    "callback_every = 500\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT SETUP\")\n",
    "print(f\"  Deep Ensemble: {n_ensemble_runs} runs × 1 particle each\")\n",
    "print(f\"  SVGD: 1 run × {n_particles_svgd} particles\")\n",
    "print(f\"  Training steps: {n_steps}\")\n",
    "print(f\"  Variables: {data.x.shape[1]}\")\n",
    "print(f\"  Training samples: {data.x.shape[0]}\")\n",
    "print(f\"  Test samples: {data.x_ho.shape[0]}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Storage for results\n",
    "ensemble_results = []\n",
    "ensemble_metrics = {\n",
    "    'eshd_empirical': [],\n",
    "    'auroc_empirical': [],\n",
    "    'negll_empirical': [],\n",
    "    'eshd_mixture': [],\n",
    "    'auroc_mixture': [],\n",
    "    'negll_mixture': [],\n",
    "    'training_time': []\n",
    "}\n",
    "\n",
    "# NEW: Storage for true ensemble (combining all samples)\n",
    "all_ensemble_gs = []\n",
    "all_ensemble_thetas = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEEP ENSEMBLE APPROACH (20 runs × 1 particle)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Deep Ensemble: 20 runs with 1 particle each\n",
    "for run_idx in range(n_ensemble_runs):\n",
    "    print(f\"\\nRun {run_idx + 1}/{n_ensemble_runs}\")\n",
    "    \n",
    "    # Use different seed for each run\n",
    "    key, subk = random.split(key)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create DiBS instance\n",
    "    dibs = JointDiBS(\n",
    "        x=data.x, \n",
    "        interv_mask=None, \n",
    "        graph_model=graph_model, \n",
    "        likelihood_model=likelihood_model\n",
    "    )\n",
    "    \n",
    "    # Sample with 1 particle\n",
    "    gs, thetas = dibs.sample(\n",
    "        key=subk, \n",
    "        n_particles=1, \n",
    "        steps=n_steps, \n",
    "        callback_every=callback_every\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Get distributions\n",
    "    dibs_empirical = dibs.get_empirical(gs, thetas)\n",
    "    dibs_mixture = dibs.get_mixture(gs, thetas)\n",
    "    \n",
    "    # Compute metrics\n",
    "    # Empirical\n",
    "    eshd_emp = expected_shd(dist=dibs_empirical, g=data.g)\n",
    "    auroc_emp = threshold_metrics(dist=dibs_empirical, g=data.g)['roc_auc']\n",
    "    negll_emp = neg_ave_log_likelihood(\n",
    "        dist=dibs_empirical, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Mixture\n",
    "    eshd_mix = expected_shd(dist=dibs_mixture, g=data.g)\n",
    "    auroc_mix = threshold_metrics(dist=dibs_mixture, g=data.g)['roc_auc']\n",
    "    negll_mix = neg_ave_log_likelihood(\n",
    "        dist=dibs_mixture, \n",
    "        eltwise_log_likelihood=dibs.eltwise_log_likelihood_observ, \n",
    "        x=data.x_ho\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    run_result = {\n",
    "        'run_idx': run_idx,\n",
    "        'eshd_empirical': eshd_emp,\n",
    "        'auroc_empirical': auroc_emp,\n",
    "        'negll_empirical': negll_emp,\n",
    "        'eshd_mixture': eshd_mix,\n",
    "        'auroc_mixture': auroc_mix,\n",
    "        'negll_mixture': negll_mix,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    ensemble_results.append(run_result)\n",
    "    \n",
    "    # Also store in lists for easy aggregation\n",
    "    ensemble_metrics['eshd_empirical'].append(eshd_emp)\n",
    "    ensemble_metrics['auroc_empirical'].append(auroc_emp)\n",
    "    ensemble_metrics['negll_empirical'].append(negll_emp)\n",
    "    ensemble_metrics['eshd_mixture'].append(eshd_mix)\n",
    "    ensemble_metrics['auroc_mixture'].append(auroc_mix)\n",
    "    ensemble_metrics['negll_mixture'].append(negll_mix)\n",
    "    ensemble_metrics['training_time'].append(training_time)\n",
    "    \n",
    "    # NEW: Store samples for true ensemble\n",
    "    all_ensemble_gs.append(gs)\n",
    "    all_ensemble_thetas.append(thetas)\n",
    "    \n",
    "    print(f\"  Empirical - E-SHD: {eshd_emp:5.2f}, AUROC: {auroc_emp:5.3f}, NegLL: {negll_emp:6.2f}\")\n",
    "    print(f\"  Mixture   - E-SHD: {eshd_mix:5.2f}, AUROC: {auroc_mix:5.3f}, NegLL: {negll_mix:6.2f}\")\n",
    "    print(f\"  Time: {training_time:.1f}s\")\n",
    "\n",
    "# NEW: Compute TRUE ENSEMBLE by combining ALL samples from all runs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRUE DEEP ENSEMBLE (combining all 20 samples)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine all graphs and parameters from all runs into single arrays\n",
    "combined_gs = np.concatenate(all_ensemble_gs, axis=0)  # [20, d, d] \n",
    "combined_thetas = jax.tree_map(lambda *arrays: np.concatenate(arrays, axis=0), *all_ensemble_thetas)\n",
    "\n",
    "print(f\"Combined ensemble contains {combined_gs.shape[0]} total samples\")\n",
    "\n",
    "# Create a single DiBS instance to compute distributions (any will work since we're just using the method)\n",
    "dibs_for_ensemble = JointDiBS(\n",
    "    x=data.x, \n",
    "    interv_mask=None, \n",
    "    graph_model=graph_model, \n",
    "    likelihood_model=likelihood_model\n",
    ")\n",
    "\n",
    "# Get true ensemble distributions\n",
    "true_ensemble_empirical = dibs_for_ensemble.get_empirical(combined_gs, combined_thetas)\n",
    "true_ensemble_mixture = dibs_for_ensemble.get_mixture(combined_gs, combined_thetas)\n",
    "\n",
    "# Compute metrics on true ensemble\n",
    "true_eshd_emp = expected_shd(dist=true_ensemble_empirical, g=data.g)\n",
    "true_auroc_emp = threshold_metrics(dist=true_ensemble_empirical, g=data.g)['roc_auc']\n",
    "true_negll_emp = neg_ave_log_likelihood(\n",
    "    dist=true_ensemble_empirical, \n",
    "    eltwise_log_likelihood=dibs_for_ensemble.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "true_eshd_mix = expected_shd(dist=true_ensemble_mixture, g=data.g)\n",
    "true_auroc_mix = threshold_metrics(dist=true_ensemble_mixture, g=data.g)['roc_auc']\n",
    "true_negll_mix = neg_ave_log_likelihood(\n",
    "    dist=true_ensemble_mixture, \n",
    "    eltwise_log_likelihood=dibs_for_ensemble.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "print(f\"TRUE ENSEMBLE Results:\")\n",
    "print(f\"  Empirical - E-SHD: {true_eshd_emp:5.2f}, AUROC: {true_auroc_emp:5.3f}, NegLL: {true_negll_emp:6.2f}\")\n",
    "print(f\"  Mixture   - E-SHD: {true_eshd_mix:5.2f}, AUROC: {true_auroc_mix:5.3f}, NegLL: {true_negll_mix:6.2f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SVGD APPROACH (1 run × 20 particles)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SVGD: 1 run with 20 particles\n",
    "key, subk = random.split(key)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dibs_svgd = JointDiBS(\n",
    "    x=data.x, \n",
    "    interv_mask=None, \n",
    "    graph_model=graph_model, \n",
    "    likelihood_model=likelihood_model\n",
    ")\n",
    "\n",
    "gs_svgd, thetas_svgd = dibs_svgd.sample(\n",
    "    key=subk, \n",
    "    n_particles=n_particles_svgd, \n",
    "    steps=n_steps, \n",
    "    callback_every=callback_every\n",
    ")\n",
    "\n",
    "svgd_training_time = time.time() - start_time\n",
    "\n",
    "# Get distributions\n",
    "svgd_empirical = dibs_svgd.get_empirical(gs_svgd, thetas_svgd)\n",
    "svgd_mixture = dibs_svgd.get_mixture(gs_svgd, thetas_svgd)\n",
    "\n",
    "# Compute metrics\n",
    "# Empirical\n",
    "svgd_eshd_emp = expected_shd(dist=svgd_empirical, g=data.g)\n",
    "svgd_auroc_emp = threshold_metrics(dist=svgd_empirical, g=data.g)['roc_auc']\n",
    "svgd_negll_emp = neg_ave_log_likelihood(\n",
    "    dist=svgd_empirical, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "# Mixture\n",
    "svgd_eshd_mix = expected_shd(dist=svgd_mixture, g=data.g)\n",
    "svgd_auroc_mix = threshold_metrics(dist=svgd_mixture, g=data.g)['roc_auc']\n",
    "svgd_negll_mix = neg_ave_log_likelihood(\n",
    "    dist=svgd_mixture, \n",
    "    eltwise_log_likelihood=dibs_svgd.eltwise_log_likelihood_observ, \n",
    "    x=data.x_ho\n",
    ")\n",
    "\n",
    "print(f\"SVGD Results:\")\n",
    "print(f\"  Empirical - E-SHD: {svgd_eshd_emp:5.2f}, AUROC: {svgd_auroc_emp:5.3f}, NegLL: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Mixture   - E-SHD: {svgd_eshd_mix:5.2f}, AUROC: {svgd_auroc_mix:5.3f}, NegLL: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Time: {svgd_training_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute statistics for ensemble\n",
    "def compute_stats(values):\n",
    "    return {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values),\n",
    "        'median': np.median(values)\n",
    "    }\n",
    "\n",
    "print(\"\\nDEEP ENSEMBLE STATISTICS - AVERAGE OF INDIVIDUALS (20 runs × 1 particle):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for metric_name in ['eshd_empirical', 'auroc_empirical', 'negll_empirical', \n",
    "                   'eshd_mixture', 'auroc_mixture', 'negll_mixture']:\n",
    "    stats = compute_stats(ensemble_metrics[metric_name])\n",
    "    print(f\"{metric_name:15s}: {stats['mean']:6.2f} ± {stats['std']:5.2f} \"\n",
    "          f\"[{stats['min']:5.2f}, {stats['max']:5.2f}] (median: {stats['median']:5.2f})\")\n",
    "\n",
    "training_stats = compute_stats(ensemble_metrics['training_time'])\n",
    "print(f\"{'training_time':15s}: {training_stats['mean']:6.1f} ± {training_stats['std']:5.1f}s \"\n",
    "      f\"(total: {sum(ensemble_metrics['training_time']):.1f}s)\")\n",
    "\n",
    "print(f\"\\nTRUE DEEP ENSEMBLE STATISTICS (combined 20 samples):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'eshd_empirical':15s}: {true_eshd_emp:6.2f}\")\n",
    "print(f\"{'auroc_empirical':15s}: {true_auroc_emp:6.2f}\")\n",
    "print(f\"{'negll_empirical':15s}: {true_negll_emp:6.2f}\")\n",
    "print(f\"{'eshd_mixture':15s}: {true_eshd_mix:6.2f}\")\n",
    "print(f\"{'auroc_mixture':15s}: {true_auroc_mix:6.2f}\")\n",
    "print(f\"{'negll_mixture':15s}: {true_negll_mix:6.2f}\")\n",
    "\n",
    "print(f\"\\nSVGD RESULTS (1 run × 20 particles):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'eshd_empirical':15s}: {svgd_eshd_emp:6.2f}\")\n",
    "print(f\"{'auroc_empirical':15s}: {svgd_auroc_emp:6.2f}\")\n",
    "print(f\"{'negll_empirical':15s}: {svgd_negll_emp:6.2f}\")\n",
    "print(f\"{'eshd_mixture':15s}: {svgd_eshd_mix:6.2f}\")\n",
    "print(f\"{'auroc_mixture':15s}: {svgd_auroc_mix:6.2f}\")\n",
    "print(f\"{'negll_mixture':15s}: {svgd_negll_mix:6.2f}\")\n",
    "print(f\"{'training_time':15s}: {svgd_training_time:6.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare approaches\n",
    "print(\"\\nEMPIRICAL DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"(A) AVERAGE-OF-INDIVIDUALS vs SVGD:\")\n",
    "ensemble_mean_eshd_emp = np.mean(ensemble_metrics['eshd_empirical'])\n",
    "ensemble_mean_auroc_emp = np.mean(ensemble_metrics['auroc_empirical'])\n",
    "ensemble_mean_negll_emp = np.mean(ensemble_metrics['negll_empirical'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_emp:5.2f} (± {np.std(ensemble_metrics['eshd_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_emp:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_emp - svgd_eshd_emp:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_emp:5.3f} (± {np.std(ensemble_metrics['auroc_empirical']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_emp:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_emp - svgd_auroc_emp:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_emp:6.2f} (± {np.std(ensemble_metrics['negll_empirical']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_emp - svgd_negll_emp:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\n(B) TRUE-ENSEMBLE vs SVGD:\")\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  True Ensemble: {true_eshd_emp:5.2f}\")\n",
    "print(f\"  SVGD:          {svgd_eshd_emp:5.2f}\")\n",
    "print(f\"  Difference:    {true_eshd_emp - svgd_eshd_emp:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  True Ensemble: {true_auroc_emp:5.3f}\")\n",
    "print(f\"  SVGD:          {svgd_auroc_emp:5.3f}\")\n",
    "print(f\"  Difference:    {true_auroc_emp - svgd_auroc_emp:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  True Ensemble: {true_negll_emp:6.2f}\")\n",
    "print(f\"  SVGD:          {svgd_negll_emp:6.2f}\")\n",
    "print(f\"  Difference:    {true_negll_emp - svgd_negll_emp:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\nMIXTURE DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"(A) AVERAGE-OF-INDIVIDUALS vs SVGD:\")\n",
    "ensemble_mean_eshd_mix = np.mean(ensemble_metrics['eshd_mixture'])\n",
    "ensemble_mean_auroc_mix = np.mean(ensemble_metrics['auroc_mixture'])\n",
    "ensemble_mean_negll_mix = np.mean(ensemble_metrics['negll_mixture'])\n",
    "\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_eshd_mix:5.2f} (± {np.std(ensemble_metrics['eshd_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_eshd_mix:5.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_eshd_mix - svgd_eshd_mix:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_auroc_mix:5.3f} (± {np.std(ensemble_metrics['auroc_mixture']):.3f})\")\n",
    "print(f\"  SVGD:          {svgd_auroc_mix:5.3f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_auroc_mix - svgd_auroc_mix:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  Deep Ensemble: {ensemble_mean_negll_mix:6.2f} (± {np.std(ensemble_metrics['negll_mixture']):.2f})\")\n",
    "print(f\"  SVGD:          {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Difference:    {ensemble_mean_negll_mix - svgd_negll_mix:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(\"\\n(B) TRUE-ENSEMBLE vs SVGD:\")\n",
    "print(f\"Expected SHD:\")\n",
    "print(f\"  True Ensemble: {true_eshd_mix:5.2f}\")\n",
    "print(f\"  SVGD:          {svgd_eshd_mix:5.2f}\")\n",
    "print(f\"  Difference:    {true_eshd_mix - svgd_eshd_mix:+5.2f} (negative is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nAUROC:\")\n",
    "print(f\"  True Ensemble: {true_auroc_mix:5.3f}\")\n",
    "print(f\"  SVGD:          {svgd_auroc_mix:5.3f}\")\n",
    "print(f\"  Difference:    {true_auroc_mix - svgd_auroc_mix:+5.3f} (positive is better for ensemble)\")\n",
    "\n",
    "print(f\"\\nNegative Log-Likelihood:\")\n",
    "print(f\"  True Ensemble: {true_negll_mix:6.2f}\")\n",
    "print(f\"  SVGD:          {svgd_negll_mix:6.2f}\")\n",
    "print(f\"  Difference:    {true_negll_mix - svgd_negll_mix:+6.2f} (negative is better for ensemble)\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average-of-individuals comparison\n",
    "better_empirical_avg = []\n",
    "better_mixture_avg = []\n",
    "\n",
    "if ensemble_mean_eshd_emp < svgd_eshd_emp:\n",
    "    better_empirical_avg.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_emp > svgd_auroc_emp:\n",
    "    better_empirical_avg.append(\"AUROC\")\n",
    "if ensemble_mean_negll_emp < svgd_negll_emp:\n",
    "    better_empirical_avg.append(\"NegLL\")\n",
    "\n",
    "if ensemble_mean_eshd_mix < svgd_eshd_mix:\n",
    "    better_mixture_avg.append(\"E-SHD\")\n",
    "if ensemble_mean_auroc_mix > svgd_auroc_mix:\n",
    "    better_mixture_avg.append(\"AUROC\")\n",
    "if ensemble_mean_negll_mix < svgd_negll_mix:\n",
    "    better_mixture_avg.append(\"NegLL\")\n",
    "\n",
    "# True ensemble comparison\n",
    "better_empirical_true = []\n",
    "better_mixture_true = []\n",
    "\n",
    "if true_eshd_emp < svgd_eshd_emp:\n",
    "    better_empirical_true.append(\"E-SHD\")\n",
    "if true_auroc_emp > svgd_auroc_emp:\n",
    "    better_empirical_true.append(\"AUROC\")\n",
    "if true_negll_emp < svgd_negll_emp:\n",
    "    better_empirical_true.append(\"NegLL\")\n",
    "\n",
    "if true_eshd_mix < svgd_eshd_mix:\n",
    "    better_mixture_true.append(\"E-SHD\")\n",
    "if true_auroc_mix > svgd_auroc_mix:\n",
    "    better_mixture_true.append(\"AUROC\")\n",
    "if true_negll_mix < svgd_negll_mix:\n",
    "    better_mixture_true.append(\"NegLL\")\n",
    "\n",
    "print(f\"AVERAGE-OF-INDIVIDUALS Deep Ensemble outperforms SVGD on:\")\n",
    "print(f\"  Empirical distribution: {better_empirical_avg if better_empirical_avg else 'None'}\")\n",
    "print(f\"  Mixture distribution:   {better_mixture_avg if better_mixture_avg else 'None'}\")\n",
    "\n",
    "print(f\"\\nTRUE Deep Ensemble outperforms SVGD on:\")\n",
    "print(f\"  Empirical distribution: {better_empirical_true if better_empirical_true else 'None'}\")\n",
    "print(f\"  Mixture distribution:   {better_mixture_true if better_mixture_true else 'None'}\")\n",
    "\n",
    "total_ensemble_time = sum(ensemble_metrics['training_time'])\n",
    "print(f\"\\nComputational efficiency:\")\n",
    "print(f\"  Deep Ensemble total time: {total_ensemble_time:.1f}s\")\n",
    "print(f\"  SVGD time:               {svgd_training_time:.1f}s\")\n",
    "print(f\"  Time ratio (Ensemble/SVGD): {total_ensemble_time/svgd_training_time:.1f}x\")\n",
    "\n",
    "print(f\"\\nIMPORTANT: This comparison demonstrates two different ways to use deep ensembles:\")\n",
    "print(f\"  - AVERAGE-OF-INDIVIDUALS: Average the performance metrics across runs\")\n",
    "print(f\"  - TRUE ENSEMBLE: Combine all samples into one distribution, then evaluate\")\n",
    "print(f\"  - SVGD: Particle interaction for Bayesian inference\")\n",
    "print(f\"\\nThe TRUE ENSEMBLE approach is the proper way to evaluate ensemble methods!\")\n",
    "print(f\"Averaging individual performances != Performance of the ensemble!\")\n",
    "\n",
    "# Save results for further analysis\n",
    "results_dict = {\n",
    "    'ensemble_results': ensemble_results,\n",
    "    'ensemble_metrics': ensemble_metrics,\n",
    "    'true_ensemble_results': {\n",
    "        'eshd_empirical': true_eshd_emp,\n",
    "        'auroc_empirical': true_auroc_emp,\n",
    "        'negll_empirical': true_negll_emp,\n",
    "        'eshd_mixture': true_eshd_mix,\n",
    "        'auroc_mixture': true_auroc_mix,\n",
    "        'negll_mixture': true_negll_mix,\n",
    "        'combined_samples': combined_gs.shape[0]\n",
    "    },\n",
    "    'svgd_results': {\n",
    "        'eshd_empirical': svgd_eshd_emp,\n",
    "        'auroc_empirical': svgd_auroc_emp,\n",
    "        'negll_empirical': svgd_negll_emp,\n",
    "        'eshd_mixture': svgd_eshd_mix,\n",
    "        'auroc_mixture': svgd_auroc_mix,\n",
    "        'negll_mixture': svgd_negll_mix,\n",
    "        'training_time': svgd_training_time\n",
    "    },\n",
    "    'ground_truth_edges': np.sum(data.g),\n",
    "    'experiment_params': {\n",
    "        'n_ensemble_runs': n_ensemble_runs,\n",
    "        'n_particles_svgd': n_particles_svgd,\n",
    "        'n_steps': n_steps,\n",
    "        'n_vars': data.x.shape[1],\n",
    "        'n_train_samples': data.x.shape[0],\n",
    "        'n_test_samples': data.x_ho.shape[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nResults stored in 'results_dict' variable for further analysis.\")\n",
    "print(f\"Individual ensemble runs available in 'ensemble_results' list.\")\n",
    "print(f\"Ensemble aggregated metrics available in 'ensemble_metrics' dict.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETED\")\n",
    "print(\"=\"*60) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06868607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensemble Experiments for Causal Discovery with Interventional Data\n",
    "\n",
    "This script compares SVGD and Deep Ensembles on their ability to recover\n",
    "a causal graph when trained and evaluated on a mix of observational and\n",
    "interventional data.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.random as random\n",
    "import jax.tree_util\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "\n",
    "from dibs.target import make_synthetic_bayes_net, make_graph_model\n",
    "from dibs.models import DenseNonlinearGaussian\n",
    "from dibs.inference import JointDiBS\n",
    "from dibs.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "from dibs.utils import visualize_ground_truth\n",
    "\n",
    "# # Setup\n",
    "key = random.PRNGKey(42)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# ## 1. Generate Ground Truth Data (with Interventions)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. GENERATING GROUND TRUTH DATA (WITH INTERVENTIONS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N_VARS = 20\n",
    "N_OBSERVATIONS = 100\n",
    "N_HO_OBSERVATIONS = 100\n",
    "N_INTERVENTION_SETS = 10 # Number of different intervention targets\n",
    "PERC_INTERVENED = 0.1 # Percentage of nodes to intervene on in each set\n",
    "\n",
    "key, subk = random.split(key)\n",
    "\n",
    "# Define graph and likelihood models\n",
    "graph_model = make_graph_model(n_vars=N_VARS, graph_prior_str=\"sf\")\n",
    "likelihood_model = DenseNonlinearGaussian(\n",
    "    n_vars=N_VARS,\n",
    "    hidden_layers=(5,),\n",
    "    obs_noise=0.1,\n",
    "    sig_param=1.0,\n",
    ")\n",
    "\n",
    "# Generate data using the base function to get interventional data\n",
    "data = make_synthetic_bayes_net(\n",
    "    key=subk,\n",
    "    n_vars=N_VARS,\n",
    "    graph_model=graph_model,\n",
    "    generative_model=likelihood_model,\n",
    "    n_observations=N_OBSERVATIONS,\n",
    "    n_ho_observations=N_HO_OBSERVATIONS,\n",
    "    n_intervention_sets=N_INTERVENTION_SETS,\n",
    "    perc_intervened=PERC_INTERVENED,\n",
    ")\n",
    "\n",
    "print(f\"Ground truth graph has {np.sum(data.g)} edges\")\n",
    "\n",
    "# Create the full training and testing datasets with masks\n",
    "# Training data: observational + first half of each interventional set\n",
    "x_train_parts = [data.x]\n",
    "mask_train_parts = [np.zeros_like(data.x, dtype=int)]\n",
    "\n",
    "# Testing data: held-out observational + second half of each interventional set\n",
    "x_ho_parts = [data.x_ho]\n",
    "mask_ho_parts = [np.zeros_like(data.x_ho, dtype=int)]\n",
    "\n",
    "for interv_dict, x_interv_full in data.x_interv:\n",
    "    # Split interventional data\n",
    "    x_interv_train, x_interv_ho = np.split(x_interv_full, 2)\n",
    "    \n",
    "    # Create mask for this intervention\n",
    "    mask = np.zeros_like(x_interv_full, dtype=int)\n",
    "    for target_node in interv_dict.keys():\n",
    "        mask[:, target_node] = 1\n",
    "    mask_train, mask_ho = np.split(mask, 2)\n",
    "\n",
    "    x_train_parts.append(x_interv_train)\n",
    "    mask_train_parts.append(mask_train)\n",
    "    x_ho_parts.append(x_interv_ho)\n",
    "    mask_ho_parts.append(mask_ho)\n",
    "\n",
    "# Combine all parts into final datasets\n",
    "x_train = np.vstack(x_train_parts)\n",
    "mask_train = np.vstack(mask_train_parts)\n",
    "x_ho = np.vstack(x_ho_parts)\n",
    "mask_ho = np.vstack(mask_ho_parts)\n",
    "\n",
    "print(f\"Total training samples: {x_train.shape[0]}\")\n",
    "print(f\"Total held-out samples: {x_ho.shape[0]}\")\n",
    "print(f\"Percentage of training samples that are interventional: {100 * (1 - data.x.shape[0] / x_train.shape[0]):.2f}%\")\n",
    "\n",
    "\n",
    "# ## 2. Experiment Parameters\n",
    "N_PARTICLES = 20\n",
    "N_ENSEMBLE_RUNS = 20\n",
    "N_STEPS = 2000\n",
    "\n",
    "\n",
    "# ## 3. SVGD Baseline (1 run × 20 particles)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"3. SVGD BASELINE (1 run x {N_PARTICLES} particles)\")\n",
    "print(\"=\"*70)\n",
    "key, subk = random.split(key)\n",
    "dibs_svgd = JointDiBS(x=x_train, interv_mask=mask_train, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "\n",
    "start_time = time.time()\n",
    "gs_svgd, thetas_svgd = dibs_svgd.sample(key=subk, n_particles=N_PARTICLES, steps=N_STEPS)\n",
    "svgd_time = time.time() - start_time\n",
    "\n",
    "svgd_empirical = dibs_svgd.get_empirical(gs_svgd, thetas_svgd)\n",
    "svgd_mixture = dibs_svgd.get_mixture(gs_svgd, thetas_svgd)\n",
    "print(f\"Finished in {svgd_time:.2f}s\")\n",
    "\n",
    "\n",
    "# ## 4. Deep Ensemble (20 runs × 1 particle)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"4. DEEP ENSEMBLE ({N_ENSEMBLE_RUNS} runs x 1 particle)\")\n",
    "print(\"=\"*70)\n",
    "ensemble_gs = []\n",
    "ensemble_thetas = []\n",
    "\n",
    "ensemble_start = time.time()\n",
    "for i in range(N_ENSEMBLE_RUNS):\n",
    "    print(f\"Run {i+1}/{N_ENSEMBLE_RUNS}\", end=\" \")\n",
    "    key, subk = random.split(key)\n",
    "    dibs_single = JointDiBS(x=x_train, interv_mask=mask_train, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "    \n",
    "    gs, thetas = dibs_single.sample(key=subk, n_particles=1, steps=N_STEPS)\n",
    "    \n",
    "    ensemble_gs.append(gs)\n",
    "    ensemble_thetas.append(thetas)\n",
    "    print(\"✓\")\n",
    "\n",
    "ensemble_time = time.time() - ensemble_start\n",
    "\n",
    "# Combine all samples for true ensemble\n",
    "combined_gs = np.concatenate(ensemble_gs, axis=0)\n",
    "combined_thetas = jax.tree_util.tree_map(lambda *arrays: np.concatenate(arrays, axis=0), *ensemble_thetas)\n",
    "\n",
    "# Create true ensemble distributions\n",
    "dibs_ensemble = JointDiBS(x=x_train, interv_mask=mask_train, graph_model=graph_model, likelihood_model=likelihood_model)\n",
    "true_ensemble_empirical = dibs_ensemble.get_empirical(combined_gs, combined_thetas)\n",
    "true_ensemble_mixture = dibs_ensemble.get_mixture(combined_gs, combined_thetas)\n",
    "print(f\"Finished in {ensemble_time:.2f}s\")\n",
    "\n",
    "\n",
    "# ## 5. Evaluation on Interventional Data\n",
    "\n",
    "# Wrapper for the interventional likelihood for evaluation\n",
    "def eltwise_log_likelihood_interv_wrapper(g, theta, x, interv_mask):\n",
    "    return dibs_svgd.eltwise_log_likelihood_interv(g, theta, x, interv_mask)\n",
    "\n",
    "def compute_metrics_interventional(dist, name, dibs_instance):\n",
    "    eshd = expected_shd(dist=dist, g=data.g)\n",
    "    auroc = threshold_metrics(dist=dist, g=data.g)['roc_auc']\n",
    "    \n",
    "    # Use a partial function to pass the held-out mask to the likelihood function\n",
    "    interv_log_likelihood_fn = functools.partial(\n",
    "        eltwise_log_likelihood_interv_wrapper, \n",
    "        interv_mask=mask_ho\n",
    "    )\n",
    "\n",
    "    negll = neg_ave_log_likelihood(\n",
    "        dist=dist, \n",
    "        eltwise_log_likelihood=interv_log_likelihood_fn, \n",
    "        x=x_ho\n",
    "    )\n",
    "\n",
    "    print(f'{name:25s} | E-SHD: {eshd:5.2f}  AUROC: {auroc:5.3f}  NegLL: {negll:7.2f}')\n",
    "    return {'eshd': eshd, 'auroc': auroc, 'negll': negll}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5. RESULTS (ON INTERVENTIONAL DATA)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# SVGD results\n",
    "svgd_emp_metrics = compute_metrics_interventional(svgd_empirical, 'SVGD Empirical', dibs_svgd)\n",
    "svgd_mix_metrics = compute_metrics_interventional(svgd_mixture, 'SVGD Mixture', dibs_svgd)\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# True ensemble results\n",
    "true_emp_metrics = compute_metrics_interventional(true_ensemble_empirical, 'Ensemble Empirical', dibs_ensemble)\n",
    "true_mix_metrics = compute_metrics_interventional(true_ensemble_mixture, 'Ensemble Mixture', dibs_ensemble)\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ## 6. Summary\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"6. SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Computation time:\")\n",
    "print(f\"  SVGD ({N_PARTICLES} particles):      {svgd_time:6.1f}s\")\n",
    "print(f\"  Deep Ensemble ({N_ENSEMBLE_RUNS} × 1):   {ensemble_time:6.1f}s\")\n",
    "print(f\"  Time Ratio (Ensemble/SVGD): {ensemble_time/svgd_time:.1f}x\")\n",
    "\n",
    "print(f\"\\nEmpirical distribution (E-SHD, lower is better):\")\n",
    "print(f\"  SVGD:          {svgd_emp_metrics['eshd']:6.2f}\")\n",
    "print(f\"  True Ensemble: {true_emp_metrics['eshd']:6.2f}\")\n",
    "\n",
    "print(f\"\\nEmpirical distribution (AUROC, higher is better):\")\n",
    "print(f\"  SVGD:          {svgd_emp_metrics['auroc']:.3f}\")\n",
    "print(f\"  True Ensemble: {true_emp_metrics['auroc']:.3f}\")\n",
    "\n",
    "print(f\"\\nMixture distribution (E-SHD, lower is better):\")\n",
    "print(f\"  SVGD:          {svgd_mix_metrics['eshd']:6.2f}\")\n",
    "print(f\"  True Ensemble: {true_mix_metrics['eshd']:6.2f}\")\n",
    "\n",
    "print(f\"\\nMixture distribution (AUROC, higher is better):\")\n",
    "print(f\"  SVGD:          {svgd_mix_metrics['auroc']:.3f}\")\n",
    "print(f\"  True Ensemble: {true_mix_metrics['auroc']:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
