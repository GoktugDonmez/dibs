{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e361100",
   "metadata": {},
   "source": [
    "## Example: Joint inference of $p(G, \\Theta | \\mathcal{D})$ for Gaussian Bayes nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d0dfc",
   "metadata": {},
   "source": [
    "Setup for Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/larslorch/dibs.git\n",
    "%cd dibs\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b52aa",
   "metadata": {},
   "source": [
    "DiBS translates the task of inferring the posterior over Bayesian networks into an inference problem over the continuous latent variable $Z$. This is achieved by modeling the directed acyclic graph $G$ of the Bayesian network using the generative model $p(G | Z)$. The prior $p(Z)$ enforces the acyclicity of $G$.\n",
    "Ultimately, this allows us to infer $p(Z, \\Theta | \\mathcal{D})$ (or $p(Z | \\mathcal{D})$) instead of $p(G, \\Theta | \\mathcal{D})$ (or $p(G | \\mathcal{D})$, respectively) using off-the-shelf inference methods, such as Stein Variational gradient descent (SVGD) (Liu and Wang, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "from jax import random\n",
    "\n",
    "from dibs.inference import JointDiBS\n",
    "from dibs.kernel import JointAdditiveFrobeniusSEKernel\n",
    "\n",
    "from dibs.eval.target import make_linear_gaussian_model, make_nonlinear_gaussian_model\n",
    "from dibs.eval.metrics import expected_shd, threshold_metrics, neg_ave_log_likelihood\n",
    "            \n",
    "from dibs.utils.graph import elwise_acyclic_constr_nograd as constraint\n",
    "from dibs.utils.func import joint_dist_to_marginal, particle_joint_empirical, particle_joint_mixture\n",
    "from dibs.utils.visualize import visualize, visualize_ground_truth\n",
    "\n",
    "from dibs.config.example import DiBSExampleSettings\n",
    "\n",
    "key = random.PRNGKey(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5e4a",
   "metadata": {},
   "source": [
    "### Generate synthetic ground truth Bayesian network and corresponding target densities for inference\n",
    "\n",
    "Below, the default synthetic Bayesian network (BN) sampled is a linear Gaussian network (for computational efficiency in this notebook). The random graph model is chosen via `graph_prior_str`, where `er` denotes Erdos-Renyi and `sf` scale-free graphs.\n",
    "\n",
    "The only change required to **infer nonlinear Gaussian networks parameterized by fully-connected neural networks is commenting in the line with `make_nonlinear_gaussian_model`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98551a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sample synthetic Bayesian network\"\"\"\n",
    "n_vars = 20\n",
    "key, subk = random.split(key)\n",
    "target = make_linear_gaussian_model(key=subk, n_vars=n_vars, graph_prior_str=\"sf\")\n",
    "# target = make_nonlinear_gaussian_model(key=subk, n_vars=n_vars, graph_prior_str=\"sf\")\n",
    "\n",
    "# train and held-out observations\n",
    "x = jnp.array(target.x)\n",
    "x_ho = jnp.array(target.x_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate target densities for posterior inference\"\"\"\n",
    "model = target.inference_model\n",
    "print(type(model).__name__)\n",
    "\n",
    "no_interv_targets = jnp.zeros(n_vars).astype(bool) # observational data\n",
    "\n",
    "def log_prior(single_w_prob):\n",
    "    \"\"\"log p(G) using edge probabilities as G\"\"\"\n",
    "    return target.graph_model.unnormalized_log_prob_soft(soft_g=single_w_prob)\n",
    "\n",
    "def log_joint_target(single_w, single_theta, rng):\n",
    "    \"\"\"log p(theta, D | G) =  log p(theta | G) + log p(D | G, theta)\"\"\"\n",
    "    log_prob_theta = model.log_prob_parameters(theta=single_theta, w=single_w)\n",
    "    log_lik = model.log_likelihood(theta=single_theta, w=single_w, data=x, interv_targets=no_interv_targets)\n",
    "    return log_prob_theta + log_lik\n",
    "\n",
    "eltwise_log_prob = vmap(lambda g, theta: log_joint_target(g, theta, None), (0, 0), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67140983",
   "metadata": {},
   "source": [
    "### Prepare DiBS\n",
    "\n",
    "Randomly initialize particles and prepare visualization.\n",
    "\n",
    "To explicitly only perform marginal posterior inference of $p(G | \\mathcal{D})$ and marginalize out the parameters in closed-form, e.g., using the BGe score, all appearances of the parameters $\\Theta$ need to be dropped. Since several function signatures change under the hood, there is a separate analogous class `dibs.svgd.marginal_dibs_svgd.MarginalDiBS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f263ccd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SVGD + DiBS hyperparams\n",
    "n_particles = 20\n",
    "n_steps = 1000\n",
    "hparams = DiBSExampleSettings()\n",
    "\n",
    "# initialize kernel and algorithm\n",
    "kernel = JointAdditiveFrobeniusSEKernel(\n",
    "    h_latent=hparams.h_latent,\n",
    "    h_theta=hparams.h_theta)\n",
    "\n",
    "dibs = JointDiBS(\n",
    "    kernel=kernel, \n",
    "    target_log_prior=log_prior,\n",
    "    target_log_joint_prob=log_joint_target,\n",
    "    alpha_linear=hparams.alpha_linear)\n",
    "        \n",
    "# initialize particles\n",
    "key, subk = random.split(key)\n",
    "init_z, init_theta = dibs.sample_initial_random_particles(\n",
    "    key=subk, model=model, n_particles=n_particles, n_vars=n_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare visualization\n",
    "images = []\n",
    "def callback(**kwargs):\n",
    "    zs = kwargs[\"zs\"]\n",
    "    gs = kwargs[\"dibs\"].particle_to_g_lim(zs)\n",
    "    probs = kwargs[\"dibs\"].edge_probs(zs, kwargs[\"t\"])\n",
    "    display.clear_output(wait=True)\n",
    "    visualize(probs,  save_path=hparams.save_path, t=kwargs[\"t\"], show=True)\n",
    "    print(\n",
    "        f'iteration {kwargs[\"t\"]:6d}'\n",
    "        f' | alpha {dibs.alpha(kwargs[\"t\"]):6.1f}'\n",
    "        f' | beta {dibs.beta(kwargs[\"t\"]):6.1f}'\n",
    "        f' | #cyclic {(constraint(kwargs[\"dibs\"].particle_to_g_lim(zs), n_vars) > 0).sum().item():3d}'\n",
    "    )\n",
    "    return\n",
    "\n",
    "# visualize ground truth (to be inferred)\n",
    "visualize_ground_truth(target.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e46b",
   "metadata": {},
   "source": [
    "### Run DiBS with SVGD\n",
    "\n",
    "Visualization shows the *matrix of edge probabilities* $G_\\alpha(Z^{(k)})$ implied by each transported latent particle (i.e., sample) $Z^{(k)}$. Refer to the paper for further details. Probabilities around 0.0 are shown in blue, around 1.0 in yellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0ece4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# iteratively transport particles \n",
    "key, subk = random.split(key)\n",
    "z, theta = dibs.sample_particles(key=subk, n_steps=n_steps, \n",
    "    init_z=init_z, init_theta=init_theta, callback_every=50, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dc8cc",
   "metadata": {},
   "source": [
    "### Evaluate on held-out data\n",
    "\n",
    "As $\\alpha \\rightarrow \\infty$, we may convert $G_\\alpha(Z)$ to $G_\\infty(Z)$ and obtain a discrete graph adjacency marix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dibs.particle_to_g_lim(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850a190",
   "metadata": {},
   "source": [
    "Form the empirical (i.e., uniformly weighted) and mixture distributions (denoted DiBS+ in the paper) based on the unnormalized posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dibs_empirical = particle_joint_empirical(g, theta)\n",
    "dibs_mixture = particle_joint_mixture(g, theta, eltwise_log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e275383",
   "metadata": {},
   "source": [
    "Compute some evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates log likelihood of all (G, theta) particles in batch on held-out data\n",
    "eltwise_log_likelihood = jit(vmap(lambda w_, theta_, x_: \\\n",
    "    (model.log_likelihood(theta=theta_, w=w_, data=x_, interv_targets=no_interv_targets)), (0, 0, None), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3dc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print metrics\n",
    "for descr, dist in [('DiBS ', dibs_empirical), ('DiBS+', dibs_mixture)]:\n",
    "    dist_marginal = joint_dist_to_marginal(dist)\n",
    "    \n",
    "    eshd = expected_shd(dist=dist_marginal, g=target.g)        \n",
    "    auroc = threshold_metrics(dist=dist_marginal, g=target.g)['roc_auc']\n",
    "    negll = neg_ave_log_likelihood(dist=dist, eltwise_log_joint_target=eltwise_log_likelihood, x=x_ho)\n",
    "    \n",
    "    print(f'{descr} |  E-SHD: {eshd:4.1f}    AUROC: {auroc:5.2f}    neg. LL {negll:5.2f}')\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bdf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
